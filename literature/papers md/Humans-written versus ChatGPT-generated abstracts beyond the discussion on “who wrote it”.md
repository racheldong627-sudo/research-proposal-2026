# Literature Review: Humans-written versus ChatGPT-generated abstracts beyond the discussion on “who wrote it”

**Source:** `papers pdf/Humans-written versus ChatGPT-generated abstracts beyond the discussion on “who wrote it”.pdf`  
**Converted:** 2025-11-05 15:15:40  
**Status:** Auto-converted from PDF using PyMuPDF  

---



## Page 1

Vol.:(0123456789)
Updates in Surgery (2025) 77:623–624 
https://doi.org/10.1007/s13304-025-02160-x
LETTER TO THE EDITOR
Humans‑written versus ChatGPT‑generated abstracts: 
beyond the discussion on “who wrote it”
Shigeki Matsubara1,2,3 
Received: 4 February 2025 / Accepted: 26 February 2025 / Published online: 3 March 2025 
© Italian Society of Surgery (SIC) 2025
Keywords  Abstract · Artificial intelligence · ChatGPT · Human · Writing
Dear Editor,
Nabata et al. [1] showed that human reviewers could not 
accurately distinguish between human-written and Chat-
GPT-generated abstracts. They discussed the merits and 
demerits of ChatGPT use in writing. The main merits are 
its time-saving and streamlining effects, while the demerits 
include ChatGPT’s inaccuracy and ethical concerns. I have 
some concerns and proposals.
First, choosing the Abstract as a study target may not be 
effective in highlighting the possible difference between the 
two. I believe that many write the Abstract after the manu-
script’s completion. For experienced writers, including 
myself, abstract writing is an almost automatic procedure, 
often done by “copy and paste” from the completed manu-
script. A manuscript consists of four parts [2]: (i) Introduc-
tion (involving the “Objective”), (ii) Methods, (iii) Results, 
and (iv) Discussion (describing the significance). A good 
Title usually reflects the “main results” and even the “signifi-
cance” [3]. In Nabata et al.’s experiment, the Title, Objec-
tive, and Results were inputted, enabling ChatGPT to access 
almost all essential components of the original article. Thus, 
writing an abstract is one of the “easiest” tasks for ChatGPT, 
because it is a semi-automatic procedure, highly suitable for 
AI. If one aims to highlight the difference between humans 
and ChatGPT, “uniquely human” parts should be chosen. 
For example, Opinion or Perspective pieces often require 
individual thoughts, feelings, and anecdotes (the so-called 
“human touch”) that ChatGPT is less likely to replicate. I 
believe that Nabata et al. chose the Abstract because of its 
brevity; however, a 300-word Opinion piece is similarly 
short while better highlighting the difference between the 
two [4].
Things are not simple. To elaborate on the concept of 
“human touch” in writing, let me briefly explain my previ-
ous studies. I experimented with whether ChatGPT could 
incorporate “human touch” in manuscripts without pre-input 
of this element. ChatGPT generated a readable manuscript 
but failed to capture this touch: a human-written manuscript 
with human touch was considered more appealing to read-
ers [5]. However, with appropriate input of human touch, 
ChatGPT generated similarly well-written Opinion Letters 
incorporating this element [4]. One could not distinguish 
human- versus ChatGPT writing even in pieces requir-
ing human touch, if one input appropriately, let alone the 
Abstract.
Second, I believe that the real demerits of ChatGPT are 
less visible than Nabata et al. described. The real, less-visi-
ble demerits include the potential decline in (i) human writ-
ing ability, (ii) the capacity to create concepts de novo, and 
(iii) the primitive human yearning for paper writing. These 
are difficult to evaluate scientifically; however, I believe that 
many experienced writers may understand my intention. As 
AI technology advances, many will find it difficult to resist 
heavy reliance on it for writing. Few will fully admit, “I only 
input bullet points and tasked ChatGPT with the entire writ-
ing.” AI detectors are—and will remain—incomplete. We, 
as authors, do not wish to live in fear of false accusations 
Letter addressing to: Nabata KJ, et al. Evaluating human ability 
to distinguish between ChatGPT-generated and original scientific 
abstracts. Updates Surg. 2025 Jan 24. doi: 10.1007/s13304-025-
02106-3.
 *	 Shigeki Matsubara 
	
matsushi@jichi.ac.jp
1	
Department of Obstetrics and Gynecology, Jichi 
Medical University, 3311‑1 Yakushiji, Shimotsuke, 
Tochigi 329‑0498, Japan
2	
Department of Obstetrics and Gynecology, Koga Red Cross 
Hospital, 1150 Shimoyama, Koga, Ibaraki 306‑0014, Japan
3	
Medical Examination Center, Ibaraki Western Medical 
Center, 555 Otsuka, Chikusei, Ibaraki 308‑0813, Japan


## Page 2

624
	
Updates in Surgery (2025) 77:623–624
of ChatGPT use. We, as readers and reviewers, do not wish 
to act as detectives, scrutinizing whether a manuscript was 
written by AI. All of this surely diminishes the “joy of writ-
ing and reading.”
Thus, my proposal is simple: allow the use of AI (Chat-
GPT) in writing. Its use is, in reality, inevitable. Acknowl-
edging this, we should focus on evaluating the “product.” 
This may not necessarily be time-saving for authors, because 
it requires extensive effort from authors to verify every detail 
and ensure that “all” content is entirely accurate. If the situa-
tion changes—for instance, with the emergence of a “perfect 
AI detector” or clear evidence of deteriorating writing abil-
ity due to ChatGPT use—this stance should be reconsidered.
I don’t, and won’t, use ChatGPT except as a final linguis-
tic checker [4, 5]. This is because ChatGPT editing “dilutes” 
my writing tone and style, which have been shaped by half a 
century of experience and genuinely belong to me. However, 
I have no intention of imposing my policy on others. If one 
values time-saving, confirms that ChatGPT indeed improves 
efficiency, and fully understands both the present and future 
demerits of AI use, one should have the freedom to use it at 
their own discretion.
I highly commend Nabata et al. for their comprehensive 
evaluation of human versus ChatGPT writing. My only sug-
gestion is to broaden the discussion and look beyond the 
visible merits and demerits. Personally, I recall a time when 
I did not have to consider whether “ChatGPT could improve 
this manuscript.” It was only two years ago!
Supplementary Information  The online version contains supplemen-
tary material available at https://​doi.​org/​10.​1007/​s13304-​025-​02160-x.
Acknowledgements  A part of the present concept was described and 
is cited.
Funding  None.
Data availability  Data sharing is not applicable to this article as no new 
data were created or analyzed in this study.
Declarations 
Conflict of interest  None.
Ethical approval  Not needed.
Informed consent  Not applicable.
Patient anonymity  Not applicable.
References
	 1.	 Nabata KJ, AlShehri Y, Mashat A, Wiseman SM (2025) Evaluat-
ing human ability to distinguish between ChatGPT-generated and 
original scientific abstracts. Updates Surg. https://​doi.​org/​10.​1007/​
s13304-​025-​02106-3
	 2.	 Matsubara S, Matsubara D (2024) A Checklist confirming whether 
a manuscript for submission adheres to the fundamentals of aca-
demic writing: a proposal. JMA J 7(2):276–278. https://​doi.​org/​
10.​31662/​jmaj.​2023-​0201
	 3.	 Matsubara S (2024) Crafting informative titles in medical arti-
cles to enhance the comprehension of the study findings. JMA J 
7(3):410–414. https://​doi.​org/​10.​31662/​jmaj.​2024-​0023
	 4.	 Matsubara S. What’s the difference between human-written manu-
scripts versus ChatGPT-generated manuscripts involving “human 
touch”? J Obstet Gynaecol Res (in press)
	 5.	 Matsubara S (2024) Humans-written versus ChatGPT-generated 
case reports. J Obstet Gynaecol Res 50(10):1995–1999. https://​
doi.​org/​10.​1111/​jog.​16078
Publisher's Note  Springer Nature remains neutral with regard to 
jurisdictional claims in published maps and institutional affiliations.


---

## Notes
- Auto-converted from PDF
- Please review and clean up formatting as needed
- Add manual annotations and summaries above this line
