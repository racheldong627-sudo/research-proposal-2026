# Literature Review: Can ChatGPT be an author. Generative AI creative writing assistance and perceptions of authorship, creatorship, responsibility, and disclosure

**Source:** `papers pdf/Can ChatGPT be an author. Generative AI creative writing assistance and perceptions of authorship, creatorship, responsibility, and disclosure.pdf`  
**Converted:** 2025-11-05 15:15:36  
**Status:** Auto-converted from PDF using PyMuPDF  

---



## Page 1

Vol.:(0123456789)
AI & SOCIETY (2025) 40:3405–3417 
https://doi.org/10.1007/s00146-024-02081-0
OPEN FORUM
Can ChatGPT be an author? Generative AI creative writing assistance 
and perceptions of authorship, creatorship, responsibility, 
and disclosure
Paul Formosa1   · Sarah Bankins2   · Rita Matulionyte3   · Omid Ghasemi4 
Received: 22 March 2024 / Accepted: 16 September 2024 / Published online: 27 September 2024 
© The Author(s) 2024
Abstract
The increasing use of Generative AI raises many ethical, philosophical, and legal issues. A key issue here is uncertainties 
about how different degrees of Generative AI assistance in the production of text impacts assessments of the human author-
ship of that text. To explore this issue, we developed an experimental mixed methods survey study (N = 602) asking par-
ticipants to reflect on a scenario of a human author receiving assistance to write a short novel as part of a 3 (high, medium, 
or low degrees of assistance) X 2 (human or AI assistant) factorial design. We found that, for a human author, the degree of 
assistance they receive matters for our assessments of their level of authorship, creatorship, and responsibility, but not what 
or who rendered that assistance, although it was more important to disclose human rather than AI assistance. However, in 
our assessments of the assisting agent, human assistants were viewed as warranting higher rates of authorship, creatorship, 
and responsibility, compared to AI assistants rendering the same level of support. These results help us to better understand 
emerging norms around collaborative human-AI generated text, with implications for other types of collaborative content 
creation.
Keywords  Generative AI · ChatGPT · Authorship · Creatorship · Responsibility · Disclosure
1  Introduction
While Generative Artificial Intelligence (GenAI) tools, such 
as ChatGPT, Stable Diffusion, and GitHub Copilot, have 
only recently been released for widespread public use, their 
popularity has already exceeded expectations. For example, 
after its public release, ChatGPT, a text-to-text AI, set a 
record for the fastest growing consumer user base (Hu and 
Hu 2023). GenAI refers to artificial intelligence systems that 
can generate from inputs a range of high-quality outputs 
in various formats, including images, text, computer code, 
and music (Epstein et al. 2023). Various GenAI apps have 
already been used to write novels (Mogg 2023), generate 
images that have won art competitions (Hurler 2023), and 
are set to transform work roles as diverse as sales and mar-
keting to software development (McKinsey 2023). However, 
these GenAI tools raise many ethical, philosophical, and 
legal issues (Dehouche 2021). Several of the most impor-
tant issues raised by humans using AI tools, such as Chat-
GPT, to help them to generate text are: who or what should 
be listed as an author of the text; who or what can count 
as having contributed to the creation of the text (Yanisky-
Ravid 2017); who or what is responsible for the contents of 
the text (Tu 2021); and when, and in what form, disclosure 
of that assistance is required (Jenkins and Lin 2023). Cur-
rently, the accelerating use of GenAI tools is outstripping 
comprehensive answers to these questions, and these ques-
tions are becoming more practically and legally important. 
Further, the norms around GenAI assistance in text produc-
tion remain emergent and poorly understood; however, we 
can draw on broader, but contextually similar, norms around 
human assistance in text creation to develop relevant insights 
(Nissenbaum 2011). To examine these issues, we ask: How 
 *	 Paul Formosa 
	
paul.formosa@mq.edu.au
1	
Department of Philosophy, Macquarie University, Sydney, 
Australia
2	
Department of Management, Macquarie University, Sydney, 
Australia
3	
School of Law, Macquarie University, Sydney, Australia
4	
Institute for Climate Risk and Response, University of New 
South Wales, Sydney, Australia


## Page 2

3406
	
AI & SOCIETY (2025) 40:3405–3417
does the degree of assistance, and whether that assistance is 
received from a human or an AI, impact assessments of the 
authorship, creatorship, and responsibility for a text, and the 
importance of disclosing that assistance?
To address this question, we developed an experimental 
mixed methods survey study (N = 602) asking participants 
to reflect on a scenario involving a human author receiving 
different degrees of assistance (high, medium, or low) from 
either a human or an AI assistant. We found that, for a human 
author, the degree of assistance matters for our assessments 
of their level of authorship, creatorship, and responsibility, 
but not what or who rendered that assistance except that it 
was more important to disclose human rather than AI assis-
tance. However, when people evaluated the assisting agent, 
human assistants were viewed as warranting higher rates 
of authorship, creatorship, and responsibility compared to 
AI assistants rendering the same level of support. These 
results help us to better understand emerging community 
views and norms around combined human-AI generated text, 
while providing a starting point for examining these issues 
in other forms of AI-assisted content generation.
2  Literature review: human–AI collaboration 
and implications for authorship
Large Language Models (LLMs), which drive applications 
like ChatGPT, are a form of GenAI that can produce plau-
sible sounding texts in various formats from user-generated 
prompts (Lund et al. 2023). GenAI is increasingly used in 
diverse fields to create or co-create content, especially in the 
creative industries. For example, an artist won a photography 
competition with a human-AI “co-creation” that was “the 
result of a complex interplay of prompt engineering, inpaint-
ing and outpainting” (Hurler 2023). In literature, there is 
increasing use of ChatGPT to produce novels, swamping 
some publishers with AI-generated content (Mogg 2023). 
GenAI is also being used in the creation of movie and televi-
sion scripts and in various aspects of music creation and pro-
duction, leading to controversies over whether work created 
exclusively by, or in collaboration with, AI should be eligi-
ble for creative awards (Edwards 2023). Such controversies 
link to our motivations for this study: how should we think 
about the authorship of co-created human and AI outputs? 
This is a significant question to explore as an accelerating 
amount of creative, academic, media, and work outputs are 
the result of an intermixing of human and AI contributions. 
This makes issues, such as the extent to which a person 
should be regarded as the author and creator of that output 
and responsible for its content, as well as whether disclo-
sure of assistance is required, of acute practical importance. 
This has implications for a range of issues, from who (or 
what) gets listed as authors on the title page of a novel or 
academic paper, to how much we blame or praise humans for 
their outputs co-created with AI, and to what are the nature 
of emergent acknowledgement practices that document AI 
assistance. Understanding public perceptions regarding who 
is an author in AI-assisted creation contexts is also important 
for ongoing policy and legal discussions, since only authors 
are entitled to exclusive rights and remunerations available 
under copyright laws.
In an academic context, there is ongoing debate about 
whether ChatGPT should be used to help write, and so be 
listed as an author on, scholarly outputs. Several published 
papers already list ChatGPT as one of the authors (Stokel-
Walker 2023), prompting publishers to clarify that Chat-
GPT, even if it makes significant scholarly contributions to 
an article, cannot be listed as an author as it cannot take 
responsibility for the content it produces or consent to the 
dissemination of its creations (Crawford et al. 2023; Lee 
2023; Van Gelder 2023). A recent journal editorial explain-
ing the removal of ChatGPT as an author from O’Connor 
and ChatGPT (2023) argued for the importance of transpar-
ency in how LLMs such as ChatGPT are used in the writing 
process (Siegerink et al. 2023).
Three main views on this question have emerged in the 
academic literature: (1) GenAI tools can be listed as an 
author under some circumstances (Jenkins and Lin 2023); 
(2) GenAI tools cannot be listed as authors under any cir-
cumstances (Hosseini Rasmussen et al. 2023); and (3) 
GenAI tools must be listed as authors if certain conditions 
are met (Miller 2023). Exemplifying the first view, Jenkins 
and Lin (2023) challenge the claim that AI cannot count as 
an author by arguing that, in the case of posthumous works 
by humans, we have a text generated by someone who both 
cannot be held responsible for its contents and cannot con-
sent to disseminate the work, since they are dead. But, even 
so, we can still have posthumous authors, so why not AI 
authors too? Instead of a blanket rejection of the possibility 
of AI authorship, Jenkins and Lin (2023) argue that “con-
tinuity” (how much of the AI generated text remains in the 
final product) and “creditworthiness” (would this degree of 
contribution from a human typically amount to authorship?) 
should determine the discipline-specific form of “credit” an 
AI should warrant. The degree of both AI assistance and 
user input are also relevant to assessments of continuity, 
such as cases where “the (user) input provided is very lim-
ited (e.g., ‘write an essay on this topic’)” (Lund et al. 2023). 
Given the complexity of these authorship issues, others 
argue that we may need to revise our “binary definitions of 
authorship … in which someone is either an author, or not” 
(van Dis et al. 2023).
In contrast, and exemplifying the second view, oth-
ers argue that “because LLMs do not have free will”, they 
cannot be held responsible for what they do, and thus it is 
always inappropriate to include ChatGPT as a co-author or 


## Page 3

3407
AI & SOCIETY (2025) 40:3405–3417	
to even “acknowledge” it, but it is still important to disclose 
interactions with LLMs (Hosseini Resnik et al. 2023). Legal 
scholarship that is grappling with these questions largely 
sits within this second view, on the basis that only humans 
can be legally recognised as authors. Under current laws it 
is widely held that, in the case of AI-generated work with 
no or only very limited human input, no one would legally 
qualify as an author of these works. This is because a human 
cannot qualify as an author in this case since they do not 
contribute to the creative expression of the work, while an 
AI cannot qualify as an author since the law requires authors 
to be human (White and Matulionyte 2020). In contrast, in 
the case of AI-assisted works where a human makes more 
substantive contributions, that human alone would be legally 
recognised as an author, at least regarding their contribu-
tions. From a normative perspective, legal commentators 
do not agree on whether copyright law should award protec-
tions to AI-generated and AI-assisted works or, if they did, 
who should be the author for the purposes of copyright law. 
For example, regarding work that is autonomously gener-
ated by AI, some legal scholars argue that AI should not 
be recognized as an author (Ginsburg and Budiardjo 2019), 
and that AI-generated work should not be protected under 
copyright at all (Sun 2022), but that protection should be 
awarded to work where AI is used as a tool and humans 
exercise sufficient creative effort in the development of 
that work. Gervais (2020) suggests a model to distinguish 
between authorless AI-generated works and protectable AI-
assisted works where the human is the only legally recog-
nized author. Rallabhandi (2023) suggests that the human 
or corporate creators of AI models should be recognized 
as authors of the works generated with the help of those AI 
models. In contrast, Yanisky-Ravid (2017) suggests that AI 
users should be the ones vested with authorship, ownership, 
and accountability for AI-generated outputs. There is also 
emerging literature on the responsibility for content gener-
ated by AI, especially regarding deep fakes (Schapiro 2020) 
and cases where AI use violates human rights or leads to the 
infringement of copyright (Land and Aronson 2020). Over-
all, from a legal perspective, since (and as long as) AI does 
not have a legal personality, it cannot be held to be legally 
accountable for the content it generates, and it will always 
be an AI developer or user (or both) who is held legally 
accountable for its outputs.
The third view takes a stronger form than the first view, 
moving beyond suggesting that AI can be an author, to 
arguing that in some circumstances it must be when cer-
tain thresholds are met. Exemplifying this third view, Miller 
(2023) argues that many publication ethics codes require 
authorship for all participants involved in drafting, revising, 
or making substantial scientific contributions to a text, which 
makes it potentially unethical not to include an LLM as an 
author where it meets these standards. Likewise, Polonsky 
and Rotman (2023) argue that ChatGPT can sometimes meet 
the four conditions specified in the International Commit-
tee of Medical Journal Editors (ICMJE) recommendations 
for authorship, namely making a “substantial contribution”, 
“drafting” or “revising” the work, giving “final approval” 
of the work, and agreeing to “be accountable” for the work 
(Polonsky and Rotman 2023).
In practice, there is evidence of lagging organisational 
standards regarding disclosure of Generative AI in text crea-
tion, despite its growing use. For example, a study explor-
ing algorithmic journalism shows how traditional views 
of humans as news authors remain strongly held, leading 
to discrepancies between the use of AI in generating news 
content, and the adaptation of crediting policies, such as the 
use of disclosure, by-lines, and attribution, to reflect its use 
(Montal and Reich 2017). More broadly, GenAI has also 
been compared to the use of human ghostwriters (Nowak-
Gruca 2022) who are commonly used for celebrity memoirs 
(Knapp and Hulbert 2017). Comparing the nascent informa-
tional and authorship norms around GenAI use with more 
established norms in contextually-similar cases, such as 
human ghostwriting, can help us to understand and evalu-
ate these emerging norms (Nissenbaum 2011). An empirical 
study of the ethics of ghostwriting found, for example, that 
85% of participants did not think it was unethical for the 
President of the United States to use a ghost speechwriter, 
but they were more ambivalent about whether the President 
should disclose that assistance (Riley and Brown 1996).
In sum, there is no consensus on the treatment of GenAI 
as an author for the text it may autonomously or collabora-
tively (with a human) generate. However, across the first and 
third views at least, there is some coalescence around the 
idea that GenAI could be an author and that a key potential 
threshold for AI authorship is the degree of assistance it pro-
vides to a human creator. Therefore, we argue that the more 
help that a human gets to generate content (i.e., higher AI 
continuity whereby much of the AI generated text remains in 
the final output), the less that human will be perceived as the 
author and creator of, and be responsible for, that content, 
and the greater their obligation to disclose that assistance. 
This leads to our first hypothesis:
H1: Receiving higher degrees of assistance will lead to 
significantly lower ratings for authorship, creatorship, 
and responsibility for the content produced, and a 
higher rating for the need to disclose that assistance.
However, who or what provides the assistance will also 
matter. It is established that people view human and algo-
rithmic labour differently (Bankins et al. 2024), including 
when doing the same task (Bankins et al. 2022). There is 
emerging empirical work that helps to uncover these views 
regarding AI authorship. For example, news stories attrib-
uted solely to a human were perceived as more credible than 


## Page 4

3408
	
AI & SOCIETY (2025) 40:3405–3417
news authored solely by an algorithm or by a human-assisted 
algorithm (Jia and Liu 2021). Several studies have also 
shown that AI-made art, music, and other creative outputs 
are considered less pleasant and less morally authentic (Jago, 
2019) than human-made works, although not all studies 
demonstrate this negative bias (Chiarella et al. 2022). There-
fore, we expect there to be differences between a human or 
a Generative AI (i.e., ChatGPT) providing that assistance. 
Thus, our second hypothesis is:
H2: There will be significant differences between 
receiving assistance from a Generative AI (e.g., Chat-
GPT) compared to receiving that assistance from a 
human in terms of ratings for authorship, creatorship, 
responsibility for the content produced, and the need 
to disclose that assistance.
3  Methods
3.1  Research design
Since we explore the influence of two factors, degree of 
assistance and the agent assisting, we employed an experi-
mental survey method (Wallander 2009). Our participants 
were asked to reflect on a scenario involving the construction 
of a creative text as part of a 3 (high, medium, or low level 
of assistance) X 2 (human or AI assistant) factorial design. 
This gave us six experimental vignettes (see Table 1, with 
completions per cell shown).
3.2  Materials
The text of all six vignettes were identical, except where 
we varied the degree of assistance and the agent assisting. 
To make the degree of assistance clear, a “high” level of 
assistance equated to all the text being written by the assis-
tant, a “medium” level of assistance equated to half the text 
being written by the assistant, and a “low” level of assistance 
equated to one page out of 50 being written by the assistant. 
An example vignette is given below, with manipulations in 
square brackets:
Lee is a published novelist. Lee is commissioned by a 
publisher to write a new fictional short story that needs 
to be about 50 pages. Lee is short on time, with a pub-
lisher’s deadline to meet, so they decide to get some 
help. Lee gets [ChatGPT, a generative AI system/a 
human assistant], to write [the novel in full/half the 
novel/the opening page of the novel] for them after giv-
ing the [AI/assistant] a few brief prompts. Lee makes a 
few very minor changes to [ChatGPT’s outputted/the 
assistant’s] text, [but doesn’t rewrite the novel them-
selves any further/and then writes the other half of the 
novel in full themselves/and then writes the rest of the 
novel in full themselves]. In the end, Lee uses about [50 
pages/25 pages/1 page] of lightly-edited text written 
by the [AI/human assistant] [ /and about 25 pages of 
text written by themselves/and about 49 pages of text 
written by themselves], for their novel. Once the novel 
is completed, Lee sends it to the publisher. The pub-
lisher is very pleased with the novel and publishes it as 
is. Lee takes full credit for the novel as sole author and 
does not at all acknowledge to their publisher or read-
ers that they received any help in creating their novel.
We chose a creative writing context to limit concerns 
about the potential inaccuracy of AI content, which could 
impact evaluations of its use in non-fiction text creation, 
and because it represents a common use case for ChatGPT.
3.3  Procedure
We obtained participants through Prolific, a widely used 
online data service platform that has been shown to pro-
vide better data quality than competing means of data col-
lection, such as MTurk and undergraduate student samples 
(Douglas et al. 2023). We received ethics approval from 
our University’s Human Research Ethics Committee (ref. 
no. 520231583852668). Our study was pre-registered with 
OSF (https://​osf.​io/​ymvd4). Informed consent was given 
by participants for this study. Each participant was ran-
domly assigned one vignette from our pool of six vignettes 
in a between-subjects design. After reading their assigned 
vignette, participants completed the measures and attention 
check detailed below.
3.4  Sample
We had 623 participants undertake our study. In line with 
our pre-registration, we removed participants who failed our 
attention check (21 participants), leaving 602 valid partici-
pants. The mean age of our sample was 33.3 (SD = 11.3), 
with 269 female, 328 male, and 5 non-binary respondents. 
Table 1   Description of the 6 vignettes
Group name
Scenario description
No. of 
comple-
tions
Hu-H
Human-high assistance level
101
Hu-M
Human-medium assistance level
101
Hu-L
Human-low assistance level
100
AI-H
AI-high assistance level
100
AI-M
AI-medium assistance level
100
AI-L
AI-low assistance level
100


## Page 5

3409
AI & SOCIETY (2025) 40:3405–3417	
Most of our sample had a Bachelor degree as their highest 
level of education (366), followed by Primary and Second-
ary school (163), and a Postgraduate degree (100).
3.5  Measures
There is substantial literature on what constitutes authorship 
(Love 2002), how researchers approach assigning author-
ship across collaborators (Nylenna et al. 2014), and the spe-
cific criteria used to determine authorship (Kakodkar and 
Bhonde 2022). However, we could not identify established 
measures that specifically capture people’s views of our key 
outcome variables. Therefore, we created targeted, succinct, 
and understandable questions to capture lay persons’ percep-
tions of creatorship, authorship, responsibility, and disclo-
sure across our scenarios, as detailed below.
The following measures were completed by participants 
after reading their assigned scenario. All four dependent 
variables used a scale ranging from 1 “Strongly disagree” 
to 7 “Strongly agree”. Participants only received questions 
about the assistant in their assigned scenario (either “Chat-
GPT” or the “human assistant”). “Lee” is the name of the 
human novelist in the scenario.
Creatorship was measured with a two Likert-scale item: 
“Lee created the novel”; “[ChatGPT/the human assistant] 
created the novel”.
Authorship was measured with a two Likert-scale item: 
“Lee should be listed as an author of the novel”; “[ChatGPT/
the human assistant] should be listed as an author of the 
novel”.
Responsibility was measured with a two Likert-scale 
item: “Lee is responsible or accountable for the contents of 
the novel”; “[ChatGPT/the human assistant] is responsible 
or accountable for the contents of the novel”.
Disclosure was measured with a single Likert-scale item: 
“Lee should publicly acknowledge to their publisher and 
readers their use of [ChatGPT/a human assistant] to help in 
the writing of the novel”. Note, there is only a single item 
for Disclosure as it did not make sense to ask whether Chat-
GPT/the human assistant should publicly acknowledge their 
provision of assistance.
Open-ended Question: Participants were also given the 
opportunity to provide qualitative data by answering the 
following question: “Can you expand on or explain your 
answers to the previous questions?”.
Demographics captured were age, gender, and highest 
level of education.
GenAI Experience: As a participant’s previous experience 
with a technology can influence their views (Formosa et al. 
2023), we controlled for this factor. We captured partici-
pants’ experience with ChatGPT by adapting a simplified 
version of the Game Technology Familiarity (GTF) scale 
(McEwan et al. 2020) consisting of the following single 
Likert-scale item ranging from 0 = “Never used”, 1 = “Very 
little experience”, to 7 = “Very experienced”. The item is: 
“How experienced are you with using ChatGPT (or other 
Generative AI chatbots)?”.
3.6  Analytical strategies
We employed a series of Analysis of Variance (ANOVA) 
models to analyse our data. Since participants responded 
to each outcome measure once for Lee and once for the 
assistant, we conducted separate analyses for each query. 
Each model incorporated one of our four measures as the 
dependent variable, with the assistant (human vs. AI), level 
of assistance (low vs. medium vs. high), and their interaction 
as predictors. While our preregistration initially specified a 
linear mixed model, due to the absence of by-subject and by-
item repetition (each subject rated one unique item in each 
condition), no random-effect structure was included in the 
models. To validate our analytical approach, we re-analysed 
the models by incorporating participants’ experience with 
ChatGPT as a covariate. The findings largely mirrored the 
main analysis, unless stated otherwise (refer to supplemen-
tary materials for full results). While not part of our preregis-
tration, we also re-analysed (at the suggestion of a reviewer) 
our models with age as a covariate. In this case, the findings 
from the main analysis remain unchanged, indicating that 
age was not a significant moderator in any of the models 
(refer to supplementary materials for full results). Addition-
ally, we conducted a series of ordinal Bayesian regressions 
to account for the ordered nature of our dependent variables, 
yielding results consistent with our frequentist approach pre-
sented here for clarity (see the supplementary materials for 
the results of the Bayesian models). The anonymous data 
and analysis scripts are accessible through OSF (https://​osf.​
io/​wgd7y/). We analysed our qualitative data by thematically 
coding responses to our open-ended question. We adopted an 
inductive approach, allowing themes to emerge from the data 
(Braun and Clarke 2006). Investigator triangulation (Carter 
et al. 2014) was achieved by two researchers coding all of the 
qualitative data together and resolving any coding disagree-
ments through discussion.
4  Results
4.1  Descriptive statistics
Participants’ ratings of authorship, creatorship, disclosure, 
and responsibility for different conditions of assistant (AI 
vs. human) and assistance (low vs. medium vs. high) are 
summarised in Table 2. As shown, participants rated Lee’s 
authorship, creatorship, and responsibility lower, and Lee’s 
need to disclose higher, when Lee received higher levels of 


## Page 6

3410
	
AI & SOCIETY (2025) 40:3405–3417
assistance, regardless of whether the assistant was a human 
or an AI. Participants rated the assistant’s authorship, crea-
torship, and responsibility higher when the assistant pro-
vided higher levels of assistance.
4.2  Inferential analyses
Initially, we present the inferential analysis for each of the 
dependent variable measures regarding Lee, followed by the 
analyses concerning the AI or human assistant.
4.2.1  Query = Lee
Authorship. The results revealed a main effect of assistance, 
F (2, 596) = 92.29, p < 0.001, ηp
2 = 0.24. As shown in the top 
panel of Fig. 1, participants rated higher authorship for a 
low level of assistance compared to medium and high levels 
of assistance (ps < 0.002), and medium compared to high 
(p < 0.001). The main effect of the assistant was non-sig-
nificant, indicating similar authorship ratings regardless of 
whether the assistant is a human or ChatGPT (p = 0.07). The 
interaction between these two predictors was non-significant 
(p = 0.54).
Creatorship. As in the previous model, we found a 
significant main effect of assistance, F (2, 596) = 137.80, 
p < 0.001, ηp
2 = 0.32. As depicted in the top row of Fig. 1, 
participants rated higher creatorship for a low level of assis-
tance compared to medium and high levels of assistance 
(ps < 0.001), and also for the medium level of assistance 
compared to the high level (p < 0.001). The main effect of 
assistant and the interaction terms were again non-significant 
(ps < 0.12).
Disclosure. The model revealed a main effect of assis-
tance, F (2, 596) = 17.38, p < 0.001, ηp
2 = 0.06. Participants 
indicated that Lee should acknowledge their use of the 
Table 2   Mean (standard deviation) of authorship, creatorship, disclo-
sure, and responsibility ratings across conditions of the study
Query
Outcome
Assistant Assistance
Low
Medium
High
Lee
Author-
ship
AI
6.09 
(1.28)
5.55 
(1.29)
4.35 (1.72)
Human
5.89 
(1.25)
5.5 (1.14)
3.98 (1.61)
Crea-
torship
AI
5.59 
(1.52)
4.82 
(1.42)
3.25 (1.63)
Human
5.44 
(1.38)
4.7 (1.34)
2.95 (1.64)
Disclo-
sure
AI
4.49 
(1.93)
5.31 (1.8)
5.63 (1.45)
Human
5.22 
(1.55)
5.6 (1.46)
5.93 (1.28)
Responsi-
bility
AI
5.94 
(1.22)
5.5 (1.4)
4.97 (1.82)
Human
6.05 
(1.03)
5.39 
(1.36)
4.73 (1.77)
Assistant Author-
ship
AI
3.25 
(1.91)
4.31 
(1.78)
4.66 (1.76)
Human
3.69 
(1.81)
5.11 
(1.59)
5.64 (1.35)
Crea-
torship
AI
2.42 
(1.57)
3.95 
(1.58)
5.45 (1.36)
Human
3.1 (1.64)
4.14 
(1.46)
5.8 (1.25)
Responsi-
bility
AI
3.11 
(1.76)
3.86 
(1.79)
4.29 (1.96)
Human
3.14 
(1.75)
4.64 
(1.37)
5.07 (1.7)
Fig. 1   The ratings for author-
ship, creatorship, disclosure, 
and responsibility across query, 
assistance, and assistant condi-
tions. Solid geometrical shapes 
denote the mean for each condi-
tion, while the surrounding 
plots portray the distributions of 
the raw data


## Page 7

3411
AI & SOCIETY (2025) 40:3405–3417	
assistant more when the level of assistance was high com-
pared to medium (p = 0.04) or low (p < 0.001), and when the 
level of assistance was medium compared to low (p < 0.001). 
In contrast to the previous models, the results revealed a 
main effect of assistant, F (1, 596) = 11.43, p < 0.001, 
ηp
2 = 0.02. Participants suggested that Lee should acknowl-
edge the use of assistance more when the assistant was a 
human (Mean = 5.58) compared to ChatGPT (Mean = 5.14). 
There was no significant interaction between assistance and 
assistant (p = 0.29).
Responsibility. The results revealed a main effect of 
assistance, F (2, 596) = 30.85, p < 0.001, ηp
2 = 0.09. Par-
ticipants tended to attribute greater responsibility to Lee 
when the level of assistance was low compared to medium 
or high (ps < 0.001), and when the level of assistance was 
medium compared to high (p < 0.001). The main effect of the 
assistant was non-significant, indicating similar responsibil-
ity ratings regardless of whether the assistant was a human 
or ChatGPT (p = 0.49). The interaction term was also non-
significant (p = 0.48).
4.2.2  Query = assistant
We now present participants’ responses to questions regard-
ing whether the assistant, rather than Lee, should be listed 
as an author, has created the novel, and is responsible for 
its contents. No questions regarding disclosure were asked. 
As depicted in Fig. 1, the pattern of results is reversed for 
the bottom row of the figure (query = assistant) compared 
to the top row (query = Lee). We now analyse each of the 
outcome measures.
Authorship. A main effect of assistance was found, F (2, 
596) = 51.93, p < 0.001, ηp
2 = 0.15, which indicates higher 
authorship ratings for the assistant when the level of assis-
tance was high compared to medium (p = 0.009) and low 
(p < 0.001), and medium compared to low (p < 0.001). We 
also found a main effect of assistant, F (1, 596) = 28.11, 
p < 0.001, ηp
2 = 0.05. This means that participants tend 
to rate authorship as higher for the human assistant 
(Mean = 4.81) compared to ChatGPT (Mean = 4.07). The 
interaction term was non-significant (p = 0.28).
Creatorship. The results revealed main effects of assis-
tance, F (2, 596) = 187.13, p < 0.001, ηp
2 = 0.39, and assis-
tant, F (1, 596) = 11.37, p < 0.001, ηp
2 = 0.02. Participants 
provided higher ratings for the assistant as the creator 
when the assistance level was high compared to medium 
and low, and when the assistance was medium compared to 
low (ps < 0.001). Moreover, participants rated the assistant 
higher as a creator when it was a human (Mean = 4.35) com-
pared to ChatGPT (Mean = 3.94). No significant interaction 
effect was found between assistance and assistant.
Responsibility. The results revealed main effects of 
assistance, F (2, 596) = 43.11, p < 0.001, ηp
2 = 0.13, and 
assistant, F (1, 596) = 14.15, p < 0.001, ηp
2 = 0.02. Par-
ticipants rated the assistant as more responsible when the 
assistance level was high compared to medium (p = 0.013) 
and low (p < 0.001), and when the assistance was medium 
compared to low (p < 0.001). Moreover, participants rated 
the assistant to be more responsible for the content of the 
novel when it was a human (Mean = 4.28) compared to Chat-
GPT (Mean = 3.75). The results also revealed a marginally 
significant interaction between assistance and assistant, F 
(2, 596) = 3.14, p = 0.044, ηp
2 = 0.01. As the bottom panel of 
Fig. 1 shows, the effect of assistance was more pronounced 
for the human assistant than the ChatGPT assistant condi-
tion. However, it is important to note that the interaction 
term became non-significant in the model that controlled 
for experience using AI as a covariate (see supplementary 
materials).
4.3  Open‑ended question: qualitative results
The qualitative open-ended comments clustered under six 
major themes, comprised of minor emergent themes, and 
reflect whether comments were positive, negative, or mixed 
about either Lee or the Assistant being a responsible and 
creditworthy author/creator. Table 3 lists descriptions, illus-
trative quotations, and relative frequency for all themes. 
Major theme frequencies are cumulative of the minor themes 
that comprise them. Only themes with at least 10 or more 
mentions in total are listed here. Passages which mention 
both Lee and the Assistant were dual coded under relevant 
themes for both Lee and the Assistant.
Overall, the most common themes raised about Lee 
(> 5%) were, positively, that they should be considered an 
author or creator of the text, they deserve credit for their 
work, and they are responsible for the work and its content; 
negatively, that they should have disclosed the assistance 
they received; and mixed, in terms of their use of prompt 
engineering. The mention of Lee using prompt engineer-
ing was classified as mixed as this was commonly used to 
indicate that Lee did indeed do something, but the degree of 
their contribution was ambiguous as someone or something 
else was using that prompt to generate content. In terms of 
the Assistant, the only common themes (> 5%) were positive 
and mirrored the top two positive themes for Lee, namely 
that the Assistant should be considered as an author or crea-
tor of the text, and they deserve credit for their work.
We can gain a more detailed understanding of the qual-
itative data by comparing the frequency of themes when 
varying either the type of assistant (AI vs. human assistant) 
or the degree of assistance (high vs. medium vs. low), as 
shown in Table 4. When comparing the type of assistant, 
the frequency of positive and negative themes about Lee 
are quite similar regardless of the type of assistant used (all 
differences less than 2%), whereas we see some larger shifts 


## Page 8

3412
	
AI & SOCIETY (2025) 40:3405–3417
Table 3   Major and minor themes from the qualitative data
Theme name: description
Illustrative quotes
Theme freq
Positive themes about Lee
Themes in favour of Lee as a responsible, creditworthy author
32%
Lee is the creator or author: Lee is the author, the creator, made significant contributions 
to the work, etc
“Lee should be listed as an author because he gave chatgpt the idea and few points on 
how to write the novel” [AI-H]
“Lee is the author. Sure, he used one page written by the assistant, but that's one page 
versus 49” [Hu-L]
16.6%
Lee deserves credit: Lee deserves full or partial credit
“they [Lee] should take most of the credit because they gave the prompts to the AI” [AI-
M]
“Lee has to have some credit as he came up with the prompts” [Hu-H]
7.5%
Lee is responsible: Lee is fully or partially responsible; Lee shares some responsibility 
or accountability
“AI is partly responsible for the editing of the novel[,] but Lee is fully responsible” [AI-
H]
“Lee is the owner of the publication and should take full responsibility for the content 
written in it” [Hu-L]
5.9%
Lee doesn’t need to disclose help: Lee does not need to acknowledge or disclose their use 
of an assistant
“no matter how much it [AI] assisted[,] it is still Lee's work and he does not need to 
acknowledge AI input” [AI-L]
“The assistant only contributed a very small part of the novel… This does not merit an 
acknowledgement” [Hu-L]
2%
Negative themes about Lee
Themes against Lee as a responsible, creditworthy author
21.2%
Lee needs to disclose help: Lee needs to acknowledge or disclose their use of an assis-
tant
“He [Lee] wouldn't have written that novel without ChatGPT so he should acknowledge 
that fact” [AI-L]
“The use of a ghost writer is legitimate, but I do think that some credit or acknowledg-
ment needs to be given” [Hu-H]
12.2%
Lee was deceptive, dishonest, or disrespectful: Lee lied, was deceptive, unethical, deceit-
ful, dishonest, etc
“it does come across as deceitful that she [Lee] chose to not reveal that she had assis-
tance” [AI-M]
“To claim 100% of the book as his [Lee’s] own work is dishonest” [Hu-L]
4.7%
Lee is not the author: Lee is not the author or not the creator of the work
“Lee should not be considered as the author of the novel. Giving prompts to an AI model 
is not the same as you being an author” [AI-H]
“Lee did not create the work because it did not come from his imagination” [Hu-H]
1.9%
Lee engaged in plagiarism: Specific mention of Lee engaging in plagiarism
“Her [Lee] not mentioning ChatGPT is plagiarism” [AI-L]
“While Lee provided prompts, the work is not all theirs and this constitutes plagiarism” 
[Hu-M]
0.9%
Lee is merely an editor: Lee just edited the work of others
“The human assistant should be listed as the author and Lee should be listed as editor” 
[Hu-H]
0.9%
Mixed themes about Lee
Ambivalent themes about Lee’s contribution
6.9%
Lee did prompt engineering: Lee did prompt engineering; Lee gave prompts, instruc-
tions, or ideas
“Lee wrote the prompts, the AI can make his job easy” [AI-L]
“Lee was the one that came up with the ideas and prompts but…the human assistant…
wrote the novel” [Hu-H]
6.9%
Positive themes about assistant
Themes in favour of the assistant as a responsible, creditworthy author
20.5%
Assistant is the creator or author: The assistant is the author, the creator, made signifi-
cant contributions, etc
“The novel was created by AI, for the most part, with only small tweaks being made by 
Lee” [AI-H]
“The assistant wrote half of the book so therefore they are technically an author also” 
[Hu-M]
11.7%


## Page 9

3413
AI & SOCIETY (2025) 40:3405–3417	
Table 3   (continued)
Theme name: description
Illustrative quotes
Theme freq
Assistant deserves credit: The assistant deserves full or partial credit
“I think chatgpt should be credited but to a very minor degree” [AI-L]
“The human assistant is the one that actually wrote the stories and therefore they should 
get the credit” [Hu-H]
7.5%
Assistant is responsible: The Assistant is fully or partially responsible; shares some 
responsibility or accountability
“Lee still had to provide the prompts so responsibility is roughly shared between Lee 
and ChatGPT” [AI-H]
“I think that since it was written half and half, both can be held responsible as authors” 
[Hu-M]
1.4%
Negative themes about assistant
Themes against the assistant as a responsible, creditworthy author
9.6%
Assistant is not responsible: The Assistant is not at all responsible or accountable
“I do not think it is possible for ChatGPT to be accountable for the contents of the novel 
as this requires some personal identification with the work” [AI-L]
“If only under Lee's name, all responsibility/credit for the content would rest solely with 
Lee” [Hu-M]
3%
Assistant is not the author: The Assistant is explicitly mentioned as not being an author
“It is a tool in the end, listing it as an author would be like listing ‘Google & I’ as an 
author” [AI-M]
“The human assistant didn't do enough to be considered the author” [Hu-L]
2.5%
Assistant deserves no credit: The Assistant does not need or deserve any credit
“there is no point in giving credit to ChatGPT” [AI-M]
“If the human assistant is a ghostwriter, and they were paid, then they should receive no 
credit for the novel” [Hu-H]
1.3%
Use of AI inappropriate in this context: Inappropriate or wrong to use AI for this pur-
pose or in this context
“Something needs to be done to safeguard genuine writers from being drowned out by 
this new wave of voiceless, nonsensical drivel people are overwhelming publishers 
with and claiming as their own” [AI-H]
1%
AI is not a person
“ChatGPT is not a person, so shouldn't be an author” [AI-H]
1%
AI just copies the work of others
“Lee is a parasite, using AI steals from other authors' work” [AI-H]
0.8%
Mixed themes about assistant
Ambivalent themes about the assistant’s contribution
9.7%
Contract in place or use of ghost writing: Mentions a contract, payment for work, agree-
ment, or ghostwriting arrangement in place
“ChatGPT is a bit like a ghost writer” [AI-M]
“If it is of mutual agreement the use of ‘ghost writers’ is a common practice. While the 
writer just receive [sic] the base payment, the author is the ‘face’ that is the selling 
point” [Hu-M]
4.3%
Assistant helped: The Assistant helped but without being an author or a mere tool
“ChatGPT assisted Lee with the intro of the story” [AI-L]
“Lee is the mastermind of the story but he needed some help to write his ideas” [Hu-L]
2.8%
Assistant is a tool: The AI is a mere tool
“The AI is only a tool in the hands of the author” [AI-M]
2.6%
Bold numbers represent the total frequency for each major theme


## Page 10

3414
	
AI & SOCIETY (2025) 40:3405–3417
in the themes about the Assistant in this regard. In particular, 
there were more positive (12.7% vs 7.8%) and less negative 
(2.7% vs 6.9%) themes for the human assistant compared 
to the AI assistant. Further, several extra negative themes 
emerged when the AI assistant was used which focused on 
the AI assistant not being a person, the inappropriateness of 
using AI in this context, and the concern that AI just copies 
the work of other (human) authors. Mixed views about Lee 
were higher for the AI assistant, as the prompt engineering 
sub-theme that comprised that code was mentioned more 
often with the AI assistant. When comparing the degree 
of assistance, a high degree of assistance led to the lowest 
frequency of positive themes and the highest frequency of 
negative and mixed themes about Lee. In contrast, medium 
or low degrees of assistance led to more positive and less 
negative themes about Lee, compared to a high degree of 
assistance. In terms of the themes raised about the assistant 
when varying degrees of assistance, the clearest shifts are in 
the condition with a low degree of assistance, where we see 
the lowest frequency of positive themes about the assistant 
and the highest frequency of mixed views.
An issue that cut across several themes is the idea that 
GenAI is just a tool (“ChatGPT is a tool created to help 
us, just like a computer or a cellphone” [AI-M]) and not a 
person, and thus not the sort of thing that can be responsible 
(“I do not think it is possible for ChatGPT to be accountable 
for the contents of the novel as this requires some personal 
identification with the work” [AI-L]), needs credit (“Since 
ChatGPT is not a person, I don’t particularly feel it’s nec-
essary for Lee to credit it” [AI-M]), or can receive com-
pensation for its work (“ChatGPT does not gain anything 
from receiving credit for the story” [AI-H]). However, while 
some participants explicitly tied personhood to authorship 
(“ChatGPT is not an author, an author is a person” [AI-
L]; “it is a norm that authors are humans” [AI-M]), other 
participants seemed comfortable positioning ChatGPT as 
an author (“ChatGPT did the bulk of the work so it should 
be listed as a co-author” [AI-H]; “Lee only gave hints and 
corrections, so ChatGPT should have been the author” [AI-
H]; “Both Lee and ChatGPT should be listed as writers of 
the novel” [AI-M]). A key difference that emerged between 
AI and human assistance is that the presence of a contrac-
tual agreement can only exist with a human assistant, and 
this impacted how participants saw the situation (“it mostly 
depends on the agreement the author made with the [human] 
assistant if he or she should be listed as an author” [Hu-M]).
Some participants worried about the broader impact of 
AI on writing (“really against the use of AI by writers, takes 
away all the skill required to become one” [AI-M]), includ-
ing its training on data produced by other writers (“using 
AI steals from other authors’ work” [AI-H]). However, oth-
ers saw it as an acceptable source of inspiration (“Asking 
ChatGPT to produce the opening page from Lee’s prompts 
is similar to Lee brainstorming his ideas with friends” [AI-
L]). The dishonesty or deceptiveness in Lee’s failure to dis-
close assistance was an important theme (“It is unethical for 
Lee, a published novelist, to take full credit for a novel that 
was only partially their creation” [Hu-M]). Reasons men-
tioned for Lee not needing to disclose assistance included 
that there may have been a contract in place with a human 
ghostwriter, the degree of assistance was too small, the use 
of AI tools do not need to be acknowledged, and the use of 
unacknowledged assistants is a common practice in the arts 
(“some famous artists have painters assistants who do the 
majority of the painting…The artist does not always credit 
them” [AI-H]).
5  Discussion
We examined how the degree of assistance and type of assis-
tant impacts assessments of authorship, creatorship, and 
responsibility for a creative text, and the importance of dis-
closing that assistance. Our first hypothesis focused on the 
impact of the degree of assistance (high, medium, or low) 
on these assessments and our second hypothesis on whether 
Table 4   Frequency of themes across conditions
Bold numbers indicate column totals for each experimental condition
Major themes
Type of assistant: AI 
assistant
Type of assistant: 
human assistant
Degree of assis-
tance: high
Degree of assis-
tance: medium
Degree of 
assistance: 
low
Positive themes about Lee
15.6%
16.5%
7.8%
13.2%
11%
Negative themes about Lee
11.4%
9.8%
9.2%
6.8%
5.1%
Mixed themes about Lee
4.9%
2.4%
3.3%
2.3%
1.3%
Positive themes about assistant
7.8%
12.7%
8.4%
9.1%
3%
Negative themes about assistant
6.9%
2.7%
3.5%
3%
3.1%
Mixed themes about assistant
4.6%
4.7%
2.5%
2.8%
4.5%
Totals
51.2%
48.8%
34.7%
37.2%
28.1%


## Page 11

3415
AI & SOCIETY (2025) 40:3405–3417	
the type of assistant (human or Generative AI) influenced 
these assessments.
Our first hypothesis was fully supported for both Lee and 
the Assistant, indicating that the degree of assistance exerted 
a significant effect on perceptions of authorship, creatorship, 
responsibility, and disclosure. When Lee received higher as 
compared to lower levels of assistance, Lee was seen as less 
of a creator and author, as less responsible for that text, and 
as having a greater need to disclose the assistance. Similarly, 
when the Assistant gave higher rather than lower levels of 
assistance, the Assistant was seen as more of a creator and 
author, and more responsible for the text. We also found 
some support for this hypothesis in our qualitative data 
where a similar pattern emerged, with higher degrees of 
assistance leading to less positive and more negative and 
mixed themes about Lee. Further, mentions about the assis-
tant being an author or creator were lowest in the low assis-
tance condition, in line with our hypothesis.
The importance of the degree of assistance received has 
implications for scholarship and practice in terms of the 
interplay between human content creators and the GenAI 
tools that they utilise (Hurler 2023; Knibbs 2022). This sug-
gests that rather than merely focusing on whether AI tools 
have been used, it is more important to focus on how much 
assistance the AI tools provided. This supports efforts at 
developing frameworks for capturing what role GenAI tools 
played in producing academic content (Cho et al. 2023), but 
also suggests the need to extend these efforts to narrative and 
other forms of creative writing, as well as the importance 
of documenting human inputs. This finding also supports a 
focus on what Jenkins and Lin (2023) call the “continuity” 
of AI text in the final product, as this is one way of capturing 
the degree of AI assistance. This conclusion is also relevant 
for legal and policy discussions on what human contribu-
tion is sufficient for them to be acknowledged as an author 
under the law.
Our second hypothesis was partially supported, with all 
variables significantly different as expected for the assistant 
(i.e. the authorship, creatorship, and responsibility of the 
human compared to AI assistant), but only the disclosure 
variable was significantly different for Lee (i.e., Lee’s need 
to disclose assistance when varying whether it was a human 
or AI assistant). When focusing on the assistant only, we 
found as expected that participants assigned higher rates of 
authorship, creatorship, and responsibility to human rather 
than AI assistants providing the same level of assistance. 
When focusing on Lee only, whether the assistant is a human 
or a Generative AI did not have a significant impact on par-
ticipants’ assessments of the extent to which Lee was an 
author, creator of, and responsible for the text, although it 
did have a significant impact on participants’ assessments of 
Lee’s need to disclose that assistance. The qualitative data 
further supported this finding, with more positive and less 
negative themes raised about the human assistant compared 
to the AI assistant. This supports various findings in the 
literature that show the presence of a “human bias” in which 
AI outputs are viewed less favourably than comparable 
human outputs (Bankins et al. 2022; Chiarella et al. 2022; 
Jia and Liu 2021). However, we significantly extend these 
findings to a new context by providing insights on percep-
tions of AI authorship, creatorship, and responsibility, which 
are dimensions that are becoming more important to evalu-
ate ethically and legally as the use of GenAI tools increases.
While participants assigned higher levels of these varia-
bles to human over AI assistants, they still seemed willing to 
assign some degree of creatorship, authorship, and respon-
sibility to AI assistants. For the creatorship, authorship, and 
responsibility variables, the means for the AI assistant in the 
medium and high conditions were greater than 4 (neutral 
point). This indicates that participants in these conditions 
agreed more than they disagreed that ChatGPT should be 
regarded as an author and creator of, and be responsible for, 
the text it generates. This has implications for the broader 
debate around whether GenAI can count as an author. In 
particular, it challenges the ‘second view’ identified in 
our literature review, that GenAI tools categorically can-
not count as authors and cannot be responsible for the work 
they produce to any extent (Polonsky and Rotman 2023), 
which is also the current dominant view in legal scholarship 
(White and Matulionyte 2020). Our participants thus offer 
some support for the first and third views from our review, 
and this suggests that we may need to move beyond “binary 
definitions of authorship” (van Dis et al. 2023). However, 
our results were clearly mixed, with some respondents offer-
ing statements that support the view that personhood is both 
essential for being an author and is something that GenAI 
tools lack (Hosseini Resnik et al. 2023). Another important 
difference between human and AI assistants that emerged 
from the qualitative data was the contractual arrangements 
that authors can have with human assistants, which is not 
an arrangement that can be directly replicated with GenAI 
technologies.
Drawing the two hypotheses together tells us that what 
matters most when assessing the authorship of a human 
writer is how much assistance they received, and not who (or 
what) provided that assistance. However, the importance of 
Lee disclosing assistance was significantly greater when the 
assistant was a human rather than a Generative AI. An expla-
nation for this finding seems to emerge from the qualitative 
data, where we find higher mentions of themes about Lee 
not having to disclose assistance in the AI assistant condi-
tion compared to the human assistant condition. One reason 
for this that emerged from the data is that some participants 
do not think the use of AI “tools” needs to be disclosed. 
Further, when participants noted that human assistance 
did not need to be acknowledged, this was typically either 


## Page 12

3416
	
AI & SOCIETY (2025) 40:3405–3417
because the degree of assistance was minimal or because of 
the presence of (assumed) contractual arrangements. This 
has implications for discussions around whether norms of 
disclosure differ for human and AI assistance (Polonsky and 
Rotman 2023), and debates about whether the standards for 
contributing to authorship should differ for humans and AI 
(Miller 2023).
6  Limitations and future research directions
Experimental vignette studies have established limita-
tions, given their hypothetical nature. We have addressed 
this through careful vignette design to ensure their external 
validity (Aviram 2012). Sampling choices can also impact 
the generalizability of results, and it would therefore be help-
ful to replicate our study with other samples. Alternative 
sampling techniques, such as using snowball and network 
samples to target specific groups, such as academic writing 
experts or working novelists, could also be considered to 
help validate the results found here (Landers and Behrend 
2015).
Future work could extend our approach to consider a 
broader range of cases. In particular, it would be useful to 
examine whether the effects found here replicate in the case 
of GenAI producing non-textual outputs, such as images, 
music, or video, and when producing non-fiction content, 
such as news articles, academic papers, or business reports, 
where the veracity of its outputs (given concerns around AI 
“hallucinations”) is more important than it is in our fictional 
novel case (Alkaissi and McFarlane 2023). Given the focus 
of much of our literature review is on academic writing, an 
extension of the current study to examine the use of GenAI 
in this specific context would be a particularly helpful area 
of future research.
7  Conclusion
The increasing importance of Generative AI raises a range 
of ethical, philosophical, and legal issues. We focused 
on examining perceptions around the degree to which a 
human writer is considered the responsible creator and 
author for content generated with differing degrees of 
assistance from a Generative AI or human assistant, as 
well as exploring when disclosure of that assistance is 
required. We found that, for a human author, the degree 
of assistance matters for our assessments of their level of 
authorship, creatorship, and responsibility, but not what 
or who rendered that assistance except that it was more 
important to disclose human rather than AI assistance. 
However, regarding types of assistants, human assis-
tants were viewed as having higher rates of authorship, 
creatorship, and responsibility compared to AI assistants 
rendering the same level of support. These results help 
us to better understand emerging norms around combined 
human-AI generated content, which has significance for a 
range of important practical and legal debates in the use of 
increasingly sophisticated GenAI technologies.
Supplementary Information  The online version contains supplemen-
tary material available at https://​doi.​org/​10.​1007/​s00146-​024-​02081-0.
Acknowledgements  We acknowledge funding support for data collec-
tion from the Macquarie University Ethics and Agency Research Cen-
tre. We thank Ed Chen for his assistance in sourcing relevant literature 
and participants at the “Workshop on AI, Arts and Copyright” for their 
feedback on an earlier version of the paper.
Funding   Open Access funding enabled and organized by CAUL and 
its Member Institutions. Macquarie University Ethics and Agency 
Research Centre.
Data availability  The pre-registration details, anonymous quantitative 
data, and analysis scripts for this study are available via OSF (https://​
osf.​io/).
Declarations 
Conflict of interest  On behalf of all authors, the corresponding author 
states that there is no conflict of interest.
Open Access   This article is licensed under a Creative Commons Attri-
bution 4.0 International License, which permits use, sharing, adapta-
tion, distribution and reproduction in any medium or format, as long 
as you give appropriate credit to the original author(s) and the source, 
provide a link to the Creative Commons licence, and indicate if changes 
were made. The images or other third party material in this article are 
included in the article's Creative Commons licence, unless indicated 
otherwise in a credit line to the material. If material is not included in 
the article's Creative Commons licence and your intended use is not 
permitted by statutory regulation or exceeds the permitted use, you will 
need to obtain permission directly from the copyright holder. To view a 
copy of this licence, visit http://​creat​iveco​mmons.​org/​licen​ses/​by/4.​0/.
References
Alkaissi H, McFarlane SI (2023) Artificial hallucinations in ChatGPT. 
Cureus. https://​doi.​org/​10.​7759/​cureus.​35179
Aviram H (2012) What would you do? Conducting web-based factorial 
vignette surveys. Handbook of survey methodology for the social 
sciences. Springer, New York
Bankins S, Formosa P, Griep Y, Richards D (2022) AI decision making 
with dignity? Contrasting workers’ justice perceptions of human 
and AI Decision making in a human resource management con-
text. Inf Syst Front 24:857–875
Braun V, Clarke V (2006) Using thematic analysis in psychology. Qual 
Res Psychol 3(2):77–101
Carter N, Bryant-Lukosius D, DiCenso A, Blythe J, Neville AJ (2014) 
The use of triangulation in qualitative research. Oncol Nurs Forum 
41(5):545–547
Chiarella SG, Torromino G, Gagliardi DM, Rossi D, Babiloni F, Car-
tocci G (2022) Investigating the negative bias towards artificial 
intelligence. Comput Hum Behav 137:107406


## Page 13

3417
AI & SOCIETY (2025) 40:3405–3417	
Cho WI, Cho E, Cho K (2023) PaperCard for reporting machine assis-
tance in academic writing. arXiv 2310.04824
Crawford J, Cowling M, Ashton-Hay S, Kelder JA, Middleton R, Wil-
son G (2023) Artificial intelligence and authorship editor policy. J 
Univ Teach Learn Pract. https://​doi.​org/​10.​53761/1.​20.5.​01
Dehouche N (2021) Plagiarism in the age of massive generative pre-
trained transformers (GPT-3). Ethics Sci Environ Polit 21:17–23
Douglas BD, Ewell PJ, Brauer M (2023) Data quality in online human-
subjects research. PLoS ONE 18(3):e0279720
Edwards B (2023) Purely AI-generated songs declared ineligible for 
Grammy awards. Ars Technica. https://​arste​chnica.​com/​infor​
mation-​techn​ology/​2023/​06/​purely-​ai-​gener​ated-​songs-​decla​red-​
ineli​gible-​for-​grammy-​awards/
Epstein Z et al (2023) Art and the science of generative AI. Science 
380(6650):1110–1111. https://​doi.​org/​10.​1126/​scien​ce.​adh44​51
Formosa P, Montefiore T, Ghasemi O, McEwan M (2023) An empiri-
cal investigation of the Gamer’s dilemma. Behav Inf Technol 
43(3):571–589
Gervais D (2020) The machine as author. Iowa Law Rev. 105
Ginsburg J, Budiardjo L (2019) Authors and machines. Berkeley Tech 
l J 34:343
Hosseini M, Rasmussen LM, Resnik DB (2023a) Using AI to write 
scholarly publications. Account Res. https://​doi.​org/​10.​1080/​
08989​621.​2023.​21685​35
Hosseini M, Resnik DB, Holmes K (2023b) The ethics of disclosing the 
use of artificial intelligence tools in writing scholarly manuscripts. 
Res Ethics. https://​doi.​org/​10.​1177/​17470​16123​11804​49
Hu K, Hu K (2023) ChatGPT sets record for fastest-growing user 
base—analyst note. Reuters. https://​www.​reute​rs.​com/​techn​
ology/​chatg​pt-​sets-​record-​faste​st-​growi​ng-​user-​base-​analy​
st-​note-​2023-​02-​01/
Hurler K (2023) AI art piece wins Sony’s photography contest, artist 
refuses the award. Gizmodo Australia. https://​gizmo​do.​com.​au/​
2023/​04/​ai-​art-​piece-​wins-​sonys-​photo​graphy-​conte​st-​artist-​refus​
es-​the-​award/
Jenkins R, Lin P (2023) AI-assisted authorship. SSRN Electron J. 
https://​doi.​org/​10.​2139/​ssrn.​43429​09
Jia C, Liu R (2021) Algorithmic or human source? Media Commun 
(Lisboa) 9(4):170
Kakodkar P, Bhonde R (2022) Authorship for interdisciplinary 
research. Medi J Dr DY Patil Univ 15(4):483
Knapp JC, Hulbert MA (2017) Ghostwriting and the ethics of authen-
ticity. Springer, Cham
Knibbs K (2022) A novelist and an AI cowrote your next cringe-
read. Wired. https://​www.​wired.​com/​story/k-​allado-​mcdow​
ell-​gpt-3-​amor-​cringe/
Land MK, Aronson JD (2020) Human rights and technology. Annu Rev 
Law Soc Sci 16(1):223–240
Landers RN, Behrend TS (2015) An inconvenient truth: arbitrary 
distinctions between organizational, mechanical turk, and other 
convenience samples. Ind Organ Psychol 8(2):142–164
Lee JY (2023) Can an artificial intelligence chatbot be the author of a 
scholarly article? J Educ Eval Health Prof 20:6
Love H (2002) Attributing authorship. Cambridge University Press
Lund BD, Wang T, Mannuru NR, Nie B, Shimray S, Wang Z (2023) 
ChatGPT and a new academic reality. J Assoc Inf Sci Technol 
74(5):570
McEwan M, Blackler A, Wyeth P, Johnson D (2020) Intuitive interac-
tion with motion controls in a tennis video game. Proceedings of 
the annual symposium on computer-human interaction in play. 
pp. 321–333
McKinsey (2023) Economic potential of generative AI. https://​www.​
mckin​sey.​com/​capab​iliti​es/​mckin​sey-​digit​al/​our-​insig​hts/​the-​
econo​mic-​poten​tial-​of-​gener​ative-​ai-​the-​next-​produ​ctivi​ty-​front​
ier
Miller R (2023) Holding large language models to account. Proceed-
ings of the AISB convention 2023; https://​aisb.​org.​uk/​wp-​conte​
nt/​uploa​ds/​2023/​05/​aisb2​023.​pdf
Mogg T (2023) ChatGPT behind influx of AI-written books on Ama-
zon. Digital trends. https://​www.​digit​altre​nds.​com/​compu​ting/​
chatg​pt-​behind-​influx-​of-​ai-​writt​en-​books-​on-​amazon/
Montal T, Reich Z (2017) I, robot. You, journalist. Who is the author? 
Digit J 5(7):829
Nissenbaum H (2011) A contextual approach to privacy online. Dae-
dalus 140(4):32–48
Nowak-Gruca A (2022) Could an artificial intelligence be a ghost-
writer? J Intellect Prop Rights 27:25–37
Nylenna M, Fagerbakk F, Kierulf P (2014) Authorship. BMC Med 
Ethics 15(1):53
Polonsky MJ, Rotman JD (2023) Should artificial intelligent agents be 
your co-author? Aust Mark J 31(2):91
Rallabhandi K (2023) The copyright authorship conundrum for works 
generated by artificial intelligence. George Wash Int Law Rev 
54(2):311–347
Riley LA, Brown SC (1996) Crafting a public image. J Bus Ethics 
15(7):711–720
S O’Connor, ChatGPT (2023) Open artificial intelligence platforms in 
nursing education. Nurse Educ Pract 66:103537
Schapiro Z (2020) Deep fakes accountability act. Boston Coll Intellect 
Prop Technol Forum 2020:1–16
Siegerink B, Pet LA, Rosendaal FR, Schoones JW (2023) ChatGPT 
as an author of academic papers is wrong and highlights the con-
cepts of accountability and contributorship. Nurse Educ Pract 
68:103599–103599
Stokel-Walker C (2023) ChatGPT listed as author on research papers. 
Nature 613(7945):620–621
Sun H (2022) Redesigning copyright protection in the era of artificial 
intelligence. Iowa Law Rev 107:1213
Tu S (2021) Use of artificial intelligence to determine copyright liabil-
ity for musical works. W Va Law Rev 123(3):835
van Dis EAM, Bollen J, Zuidema W, van Rooij R, Bockting CL (2023) 
ChatGPT: five priorities for research. Nature 614(7947):224–226
Van Gelder RN (2023) The pros and cons of artificial intelligence 
authorship in ophthalmology. Ophthalmology 130(7):670
Wallander L (2009) 25 years of factorial surveys in sociology. Soc Sci 
Res 38(3):505–520
White C, Matulionyte R (2020) Artificial intelligence painting the big-
ger picture for copyright ownership. Aust Intellect Prop J. https://​
doi.​org/​10.​2139/​ssrn.​34986​73
Yanisky-Ravid S (2017) Generating rembrandt. Mich St L Rev. 659
Publisher's Note  Springer Nature remains neutral with regard to 
jurisdictional claims in published maps and institutional affiliations.


---

## Notes
- Auto-converted from PDF
- Please review and clean up formatting as needed
- Add manual annotations and summaries above this line
