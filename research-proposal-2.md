Negotiating Authorial Voice in Linguistics PhD Theses: A Corpus-Based Study of Stance and Identity in Human and Customized Chatbot-Generated Texts

Introduction
The accelerated integration of artificial intelligence (AI)—particularly large language models (LLMs) such as ChatGPT—has begun to transform doctoral writing practices across higher education. Generative AI tools are increasingly used by English as a Foreign Language (EFL) students to enhance writing fluency, receive formative feedback, and engage with complex disciplinary genres, including the linguistics PhD thesis. However, this growing reliance on AI raises essential questions about the construction of authorial voice, originality, and academic integrity at the highest levels of scholarly communication (Hyland, 2008; Revell et al., 2024).

Authorial voice, comprising stance, identity, and rhetorical engagement, is central to advanced academic writing and to a writer’s development as an independent scholar (Ivanič, 1998; Hyland, 2005). In the context of EFL doctoral writing, students must simultaneously manage linguistic challenges, disciplinary expectations, and the pressure to produce original, authoritative research. Overlaying these complexities, the role of AI writing support is not yet clearly understood: recent studies highlight both the linguistic advantages of AI for EFL writers and significant shortcomings such as reduced rhetorical depth, limited epistemic positioning, and challenges to clear self-representation (Amirjalili et al., 2024; Berber Sardinha, 2024).

Given these tensions, it is vital to investigate how customized chatbot support influences EFL doctoral writers’ negotiation of stance and authorial identity. This study deploys a corpus-based comparative approach to examine human-authored and AI-assisted linguistics PhD thesis chapters, with a specific focus on rhetorical and lexical markers of voice and engagement. The research is motivated by the need to inform doctoral writing pedagogy and policy, providing empirically grounded recommendations for the responsible and effective integration of AI tools in high-stakes academic contexts.

Research Questions:

What rhetorical and lexical features characterize the expression of stance and authorial voice in linguistics PhD theses written by EFL students?

How do these features differ between human-authored and customized chatbot-assisted thesis chapters?

In what ways do customized chatbots mediate the negotiation of authorial identity (e.g., hedging, boosting, self-mention, engagement) in EFL doctoral writing?

What implications do these findings hold for doctoral writing pedagogy, ethical authorship, and institutional policy on AI-supported research writing?
## Literature Review Outline
Doctoral thesis writing presents an especially complex and demanding context for English as a Foreign Language (EFL) students, requiring not only the performance of disciplinary expertise but also the negotiation of individual scholarly voice (Hyland, 2010; Ivanič, 1998). At the heart of this process, authorial voice is constructed through stance and engagement—dimensions operationalized in Hyland’s (2005, 2008) framework as networks of hedges, boosters, self-mentions, and reader engagement strategies that facilitate both the claim of authority and the management of interaction with academic audiences. Research demonstrates that voice in academic writing is not only linguistically achieved but is also shaped by institutional, cultural, and disciplinary norms (Tang & John, 1999; Charles, 2003). Early work by Tang and John (1999) revealed how even ostensibly impersonal student writing reflects subtle authorial positioning, while Charles (2003) illustrated that doctoral writers strategically employ nominal structures and evaluative labels to assert epistemic alignment and disciplinary competence.

The experience of EFL doctoral students, as shown by Işık-Taş (2018) and reinforced by qualitative studies such as Ren (2023) and Othman and Lo (2023), involves continuous adjustment of voice and identity in response to shifting institutional and cross-cultural expectations. These studies confirm that the negotiation of authorial stance is a dynamic interplay of agency and constraint, with writers having to balance conformity with the conventions of the Anglophone academy and the desire to project an autonomous scholarly persona. Complementing these insights, corpus-based research (Hyland, 2008; Bao & Liu, 2022) has identified persistent disciplinary and linguistic patterns, such as the tendency among Chinese EFL doctoral writers to rely on formulaic bundles and to downplay explicit first-person reference—a strategy that distinguishes them from their Anglophone peers.

The rise of AI-powered writing assistants and large language models introduces new opportunities and disruptions in this already complex landscape. Systematic reviews and empirical studies (Abdallah et al., 2025; Deng et al., 2025; Albadarin et al., 2024) confirm that tools like ChatGPT can help EFL writers improve textual fluency, surface-level organization, and access to feedback, yet multiple high-quality comparative studies warn that AI-generated texts often fall short in enacting deep rhetorical stance and distinctive authorial identity (Amirjalili et al., 2024; Berber Sardinha, 2024; Revell et al., 2024). Moreover, detection research finds that despite AI’s increasing sophistication, its products often remain distinguishable from human writing—characterized by different patterns in rhetorical features, hedging, and engagement (Desaire et al., 2023; Gao et al., 2023). Recognizing these differences, publishers and universities now call for explicit AI-use disclosure and are re-evaluating authorship integrity standards in response to the growing hybridization of human and machine contributions (Lund & Naheem, 2024).

More recently, developments in AI customization have offered the promise of tailored writing support aligned to specific scholarly genres and disciplines, such as linguistics. However, as this capability emerges, new methodological and ethical challenges arise—especially regarding the calibration of prompts and the preservation of disciplinary authenticity (Kabir et al., 2025). Despite initial studies comparing generic AI-generated and human-authored academic texts, there remains a notable research gap in the literature. To date, no studies have systematically investigated the influence of customized chatbot support—where an AI tool is fine-tuned or purpose-built for doctoral writing in a specific field—on the construction of stance or authorial voice in PhD theses. Nor has prior research directly compared the outputs of such customized AI support to those of human-authored thesis chapters via corpus-based methods. Addressing this critical absence, the present study will be the first to design and implement a linguistics-oriented chatbot for doctoral writing, empirically contrasting its outputs with traditional thesis texts. In so doing, this research will provide timely evidence on how personalized AI may affect scholarly voice, authenticity, and the future of EFL doctoral writing pedagogy.
## Theoretical Foundation
## This study utilizes a multi-layered theoretical framework to analyze the construction of disciplinary voice and stance in linguistics PhD theses authored by native English speakers, as compared to customized AI-generated equivalents. The analysis draws upon Ivanič’s (1998) model of writer identity—focusing specifically on the discoursal self and authorial self—Hyland’s (2005, 2008) stance and engagement framework, and metadiscourse theory (Hyland, 2005; Hyland & Tse, 2004).
## Ivanič’s (1998) writer identity model provides a lens for examining how academic authors linguistically construct their represented self within written discourse. In this study, attention is directed to the discoursal self, which is revealed through the use of self-mentions, evaluative expressions, and other linguistic markers that project the writer’s persona, as well as the authorial self, which emerges in how academic authority and stance toward content and readers are linguistically enacted within the text. Given the corpus-driven nature of this research, these identity positions are traced through observable textual features rather than author biography.

## Complementing this, Hyland’s stance and engagement framework offers a detailed taxonomy for mapping the interpersonal dimensions of academic communication. Stance in this framework is marked by linguistic resources such as hedges, boosters, attitude markers, and explicit self-mentions, reflecting the author’s epistemic position, affect, and degree of alignment or detachment from claims. Engagement resources, including reader pronouns, directives, questions, and appeals, index the ways academic writers orient to and interact with their audience. By systematically identifying and quantifying these features through corpus analysis, the study compares the expression of voice and reader engagement in both native-authored and AI-generated texts.
## Metadiscourse theory further enriches the analysis by conceptualizing academic writing as an interactive, meaning-making process wherein writers both organize content and manage relationships with readers. Interactive metadiscourse—such as transitions, frame markers, and code glosses—guides the sequencing and coherence of arguments. Interactional metadiscourse—including hedges, boosters, and engagement markers—signals the writer’s stance, attitude, and openness to dialogue. In examining the frequency, distribution, and pragmatic function of these resources, the study elucidates how both human and AI-generated thesis texts achieve (or fail to achieve) clarity, credibility, and engagement.
## Together, these theoretical perspectives enable the systematic analysis of observable linguistic features within the Introduction and Discussion chapters of the corpus, providing a benchmark for disciplinary voice as constructed by expert native writers. The integrated framework thus supports not only the comparison of human- and AI-generated academic writing, but also the derivation of pedagogical insights relevant for EFL doctoral students employing AI-assisted writing tools.
## 4. Methodology
### 4.1 Research Design
This study adopts a comparative corpus-based design, integrating quantitative corpus-linguistic methods with qualitative discourse analysis. The aim is to systematically compare how authorial voice, stance, and engagement are constructed in linguistics PhD thesis sections authored by native English speakers versus texts generated by a customized AI chatbot. This dual approach allows for the identification of both broad linguistic patterns and subtle rhetorical nuances, providing a robust empirical foundation for assessing disciplinary voice in authentic and AI-assisted academic writing.
4.2 Corpus Compilation
The dataset consists of two matched sub-corpora. The first sub-corpus comprises approximately 25–30 linguistics PhD theses authored by native English speakers, sourced from reputable open-access university repositories. From each thesis, only the Introduction and Discussion chapters are selected, as these are considered the most identity-rich, argumentative, and revealing of authorial voice and stance (Swales, 2004; Hyland, 2005; Charles, 2003). Core metadata, such as year of submission and institution, are recorded to support contextual interpretation.

The second sub-corpus comprises AI-generated equivalents to each selected thesis chapter. For every thesis, a customized chatbot grounded in large language model architecture (such as GPT-4) generates a corresponding Introduction and Discussion. The prompts for the chatbot are designed to elicit native-like, genre-appropriate academic writing, and are based on each thesis's title, abstract, and research questions to maintain content fidelity and structure. Detailed logs of all prompts, AI model parameters, and generated outputs are kept, ensuring replicability and transparency.

**Sample Size Justification**

The proposed sample of 25–30 theses per condition is methodologically justified on several grounds. First, this sample size aligns with established precedent in corpus-based doctoral writing research: Işık-Taş (2008) analyzed 25 PhD theses to examine genre-specific discourse and authorial voice in ELT, while Charles (2003) identified significant disciplinary differences in stance construction with only 8 theses per corpus, publishing findings in the *Journal of English for Academic Purposes*. Second, the total projected word count—approximately 400,000–600,000 words across 50–60 chapters (Introduction and Discussion sections averaging 5,000–10,000 words each)—is comparable to or exceeds corpus sizes in published dissertation studies. For instance, while Bao and Liu (2022) analyzed 700 dissertation abstracts, these brief texts (200–300 words each) yielded a total corpus of approximately 350,000 words, similar in scale to the present study's full-chapter analysis.

Third, a priori power analysis confirms that a sample of 25–30 texts per group provides adequate statistical power (1-β = 0.80, α = 0.05) to detect large effect sizes (Cohen's d ≥ 0.8), which are typical of genre-level linguistic differences in corpus research (Hyland, 2008). Fourth, the intensive nature of this research design—requiring customized AI generation, hand-tagged discourse coding, inter-rater reliability checks, and deep qualitative interpretation of authorial identity construction—necessitates a balance between statistical adequacy and analytical depth. Specialized corpora analyzing specific genres can employ smaller samples than general corpora when texts are carefully selected for genre homogeneity and representativeness (Biber, 1993; Flowerdew, 2004). The matched-pair design, in which each human-authored thesis has a corresponding AI-generated equivalent, further enhances statistical power by controlling for content variation across texts.
4.3 Data Preparation and Coding Framework
All collected text is anonymized, converted to plain text, and cleaned of non-linguistic content such as references, figures, or tables. The coding procedure begins with automatic annotation using established keyword lists aligned with the theoretical frameworks. Markers for stance (hedges, boosters, attitude markers, self-mentions), engagement (reader pronouns, directives, questions), and both interactive and interactional metadiscourse (Hyland, 2005, 2008; Hyland & Tse, 2004) are tagged. Linguistic signals relevant to discoursal and authorial self (Ivanič, 1998), including self-positioning and authority claims, are also annotated. An initial 10% sample of all coded segments is manually checked to validate tagging accuracy, with ambiguous or borderline cases resolved through in-depth qualitative review.
4.4 Analytical Procedures
Analysis proceeds through a series of coordinated steps. First, the frequencies and distributions of all coded markers are computed for both sub-corpora. Quantitative comparison employs relevant statistical tests: Mann–Whitney U tests are used to evaluate differences in marker frequency given the anticipated non-normal distribution, while chi-square tests assess categorical distributional differences between human and AI-authored texts. Further, keyness and collocation analyses are conducted using corpus analysis software (e.g., AntConc, SketchEngine) to identify which words, bundles, and phraseologies are particularly prominent or distinctive in one corpus compared to the other.

To ensure analytical reliability and robustness, coder agreement metrics are calculated for each manually checked sample, targeting a threshold of at least 90% for annotation consistency. Statistical power is adequate for the planned analyses: with n = 25–30 per group, Mann–Whitney U tests can reliably detect medium-to-large effect sizes (d ≥ 0.5–0.8) at α = 0.05 with 80% power, which is appropriate given that genre-level comparisons typically yield large effects in corpus linguistics research.
Subsequently, selected text excerpts that best exemplify convergence or divergence in the use of stance, engagement, and metadiscourse are subject to close qualitative analysis. These extracts are interpreted to reveal how linguistic resources shape (or fail to shape) voice, epistemic stance, authority, and reader orientation in both human and AI-generated texts. Emerging themes and patterns are inductively described and related back to the frameworks.
Finally, findings from quantitative and qualitative analyses are triangulated, providing a synthesized comparative account of disciplinary voice in human and AI-authored writing. The extent to which AI-generated texts capture the rhetorical sophistication and authorial self-presentation characteristic of native-speaker doctoral writing is critically assessed, and implications for EFL AI-assisted doctoral pedagogy are highlighted.
4.5 Ethical Considerations
All data used in this study are sourced from publicly accessible open repositories or are generated in-house via controlled AI procedures. Anonymization protocols are strictly followed, and all data handling complies with relevant institutional and research ethics standards. AI-generated outputs are transparently documented and are not presented as genuine academic submissions. The project adheres to best practices in academic integrity and open science, with datasets and procedures prepared for potential future sharing in line with FAIR principles.
## 5. Implications and Contributions to Knowledge
By directly comparing human-authored and AI-generated linguistics PhD thesis chapters, this study offers several important contributions. Theoretically, it extends established models of authorial identity and stance into the domain of AI-mediated academic writing, providing new evidence on whether frameworks such as those of Hyland and Ivanič adequately capture voice and disciplinary presence in hybrid, human–machine texts. The findings will also contribute to ongoing debates about authorship, agency, and intellectual responsibility in the age of generative AI.

Methodologically, the project demonstrates a rigorous, replicable approach for corpus-based analysis of advanced academic genres, integrating quantitative and qualitative techniques suited for evaluating both native-speaker and AI-composed writing. The parallel corpus assembled for this research will serve as a resource for subsequent studies on academic voice, AI co-authorship, and digital rhetoric.

Pedagogically and ethically, this research offers guidance for doctoral writing instruction, particularly in contexts where EFL learners are increasingly encountering AI as a source of feedback or support. Clarifying how AI alters stance, engagement, and authorial self-presentation can help supervisors and students draw boundaries between appropriate assistance and overreliance, supporting academic integrity. The study’s results will inform institutional policies and AI literacy frameworks designed to safeguard responsible use of generative tools in doctoral research.

Ultimately, by benchmarking AI-generated writing against the scholarly standards set by native English-speaking doctoral authors, the study advances understanding of how human–machine collaboration is reshaping the performance of academic identity—an insight that is crucial for responsible, effective, and ethical doctoral education in a rapidly changing digital landscape.


References

Abdallah, N., Katmah, R., Khalaf, K., & Jelinek, H. F. (2025). Systematic review of ChatGPT in higher education: Navigating impact on learning, wellbeing, and collaboration. Social Sciences & Humanities Open, 12, 101866. https://doi.org/10.1016/j.ssaho.2025.101866
Biber, D. (1993). Representativeness in corpus design. Literary and Linguistic Computing, 8(4), 243–257. https://doi.org/10.1093/llc/8.4.243
Flowerdew, L. (2004). The argument for using English specialized corpora to understand academic and professional language. In U. Connor & T. A. Upton (Eds.), Discourse in the professions: Perspectives from corpus linguistics (pp. 11–33). John Benjamins.
Albadarin, Y., Saqr, M., Pope, N., & Tukiainen, M. (2024). A systematic literature 	review of empirical research on ChatGPT in education. Discover Education, 3(1), 	60. https://doi.org/10.1007/s44217-024-00138-2
Amirjalili, F., Neysani, M., & Nikbakht, A. (2024). Exploring the boundaries of 	authorship: A comparative analysis of AI-generated text and human academic 	writing in English literature. Frontiers in Education, 9, 1347421. 	https://doi.org/10.3389/feduc.2024.1347421
Bao, K., & Liu, M. (2022). A corpus study of lexical bundles used differently in 	dissertations abstracts produced by Chinese and American PhD students of 	linguistics. Frontiers in Psychology, 13, 893773.
Berber Sardinha, T. (2024). AI-generated vs human-authored texts: A multidimensional comparison. Applied Corpus Linguistics, 4(1), 100083. https://doi.org/10.1016/j.acorp.2023.100083
Charles, M. (2003). ‘This mystery…’: A corpus-based study of the use of nouns to construct stance in theses from two contrasting disciplines. Journal of English for Academic Purposes, 2(4), 313–326. https://doi.org/10.1016/S1475-1585(03)00048-1
Charles, M. (2006). The construction of stance in reporting clauses: A cross-disciplinary study of theses. Applied Linguistics, 27(3), 492–518. https://doi.org/10.1093/applin/aml021
Deng, R., Jiang, M., Yu, X., Lu, Y., & Liu, S. (2025). Does ChatGPT enhance student learning? A systematic review and meta-analysis of experimental studies. Computers & Education, 227, 105224. https://doi.org/10.1016/j.compedu.2024.105224
Desaire, H., Chua, A. E., Isom, M., Jarosova, R., & Hua, D. (2023). Distinguishing academic science writing from humans or ChatGPT with over 99% accuracy using off-the-shelf machine learning tools. Cell Reports Physical Science, 4(6), 101426. https://doi.org/10.1016/j.xcrp.2023.101426
Gao, C. A., Howard, F. M., Markov, N. S., Dyer, E. C., Ramesh, S., Luo, Y., & Pearson, A. T. (2023). Comparing scientific abstracts generated by ChatGPT to real abstracts with detectors and blinded human reviewers. Npj Digital Medicine, 6(1), 75. https://doi.org/10.1038/s41746-023-00819-6
Hyland, Ken. 2005. Metadiscourse. London: Continuum
Hyland, K. (2008). Disciplinary voices: Interactions in research writing. English text construction, 1(1), 5-22.
Hyland, K. (2005). Stance and engagement: A model of interaction in academic discourse. Discourse Studies, 7(2), 173–192.https://doi.org/10.1177/1461445605050365
Hyland, K. (2008). Academic clusters: Text patterning in published and postgraduate writing. International Journal of Applied Linguistics, 18(1), 41–62. https://doi.org/10.1111/j.1473-4192.2008.00178.x
Hyland, K., & Tse, P. (2004). Metadiscourse in academic writing: A reappraisal. Applied Linguistics, 25(2), 156–177.
Hyland, K. (2010). Community and individuality: Performing identity in applied linguistics. Written Communication, 27(2), 159–188. https://doi.org/10.1177/0741088310364939
Işık-Taş, E. E. (2008). A corpus-based analysis of genre-specific discourse of research: The PhD thesis and the research article in ELT [Doctoral dissertation, Middle East Technical University].
Ivanič, R. (1998). Writing and identity (Vol. 10). John Benjamins Publishing Company.
Kabir, A., Shah, S., Haddad, A., & Raper, D. M. S. (2025). Introducing Our Custom GPT: An Example of the Potential Impact of Personalized GPT Builders on Scientific Writing. World Neurosurgery, 193, 461–468. https://doi.org/10.1016/j.wneu.2024.10.041
Lund, B. D., & Naheem, K. T. (2024). Can ChatGPT be an author? A study of artificial intelligence authorship policies in top academic journals. Learned Publishing, 37(1), 13–21. https://doi.org/10.1002/leap.1582
Mahapatra, S. (2024). Impact of ChatGPT on ESL students’ academic writing skills: A mixed methods intervention study. Smart Learning Environments, 11(1), 9. https://doi.org/10.1186/s40561-024-00295-9
Marzuki, Widiati, U., Rusdin, D., Darwin, & Indrawati, I. (2023). The impact of AI writing tools on the content and organization of students’ writing: EFL teachers’ perspective. Cogent Education, 10(2), 2236469. https://doi.org/10.1080/2331186X.2023.2236469
Othman, J., & Lo, Y. Y. (2023). Constructing academic identity through critical argumentation: A narrative inquiry of Chinese EFL doctoral students’ experiences. SAGE Open, 13(4). https://doi.org/10.1177/21582440231200700
Ren, B. (2023). L2 writer identity construction in academic written discourse: A multi-case study (Publication No. 30491449) [Doctoral dissertation, University of South Florida]. ProQuest Dissertations and Theses Global.
Revell, T., Yeadon, W., Cahilly-Bretzin, G., Clarke, I., Manning, G., Jones, J., Mulley, C., Pascual, R. J., Bradley, N., Thomas, D., & Leneghan, F. (2024). ChatGPT versus human essayists: An exploration of the impact of artificial intelligence for authorship and academic integrity in the humanities. International Journal for Educational Integrity, 20(1), 18. https://doi.org/10.1007/s40979-024-00161-8
Tang, R., & John, S. (1999). The ‘I’ in identity: Exploring writer identity in student academic writing through the first person pronoun. English for Specific Purposes, 18, S23–S39. https://doi.org/10.1016/S0889-4906(99)00009-5
Zhang, L., & Wang, J. (2024). To be like a “scholar”: A study on the construction of authorial identity of Chinese EFL learners in academic writing—An intertextuality perspective. Frontiers in Psychology, 15, 1322851. https://doi.org/10.3389/fpsyg.2024.1322851










