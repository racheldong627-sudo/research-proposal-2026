# PhD Supervisor Feedback: Research Proposal Assessment

**Candidate**: [Anonymous]  
**Proposed Research Title**: *Negotiating Authorial Voice in Linguistics PhD Theses: A Corpus-Based Study of Stance and Identity in Human and Customized Chatbot-Generated Texts*  
**Date of Review**: November 12, 2025  
**Reviewer Role**: Potential PhD Supervisor

---

## Executive Summary

**Would I accept this candidate as my PhD student?** 

**Yes, with significant revisions required before formal registration.**

This is a **timely, innovative, and methodologically sophisticated proposal** that addresses a critical gap in applied linguistics and academic writing research. The candidate demonstrates strong theoretical grounding, awareness of current literature, and the ability to design a rigorous corpus-based study. However, several substantial issues must be resolved before this project can proceed to full PhD registration.

**Overall Assessment**: **7.5/10** (Strong proposal requiring revisions)

---

## Detailed Evaluation

### 1. Research Significance and Originality ✅ **STRONG**

**Strengths:**
- **Excellent identification of research gap**: The proposal correctly identifies that no prior studies have systematically examined customized (rather than generic) AI chatbot support on doctoral writing in linguistics
- **Timely and urgent topic**: Given the rapid adoption of AI tools in academia, this research addresses immediate pedagogical and policy needs
- **Clear contributions to knowledge**: The proposal articulates theoretical, methodological, pedagogical, and ethical contributions
- **Interdisciplinary relevance**: Bridges applied linguistics, corpus linguistics, digital rhetoric, and educational technology

**Minor Concerns:**
- Consider explicitly stating potential impact on AI tool developers and educational technologists
- Could strengthen the argument for why linguistics PhD theses specifically (vs. other disciplines) are the ideal context

**Recommendation**: Maintain current approach with minor clarifications

---

### 2. Literature Review ✅ **STRONG**

**Strengths:**
- **Comprehensive coverage**: Excellent integration of classical work (Hyland, Ivanič, Tang & John) with cutting-edge AI-in-education research
- **Critical synthesis**: Not just summarizing—actively building an argument for the research gap
- **Up-to-date references**: Includes 2024-2025 publications, showing engagement with emerging scholarship
- **Logical progression**: Moves from authorial voice theory → EFL doctoral writing challenges → AI disruption → research gap

**Areas for Enhancement:**
- **Add corpus linguistics methodology literature**: Need more explicit engagement with corpus-based discourse analysis methods (e.g., Biber, Anthony, Friginal)
- **Expand on customization technology**: Brief discussion of how AI customization works (fine-tuning vs. prompt engineering) would strengthen technical credibility
- **More on native vs. non-native writing**: Some literature on native speaker norms in academic writing would help justify the benchmark choice

**Recommendation**: Add 8-10 additional methodological and technical references

---

### 3. Theoretical Framework ⚠️ **GOOD BUT NEEDS CLARIFICATION**

**Strengths:**
- **Well-chosen frameworks**: Ivanič's writer identity + Hyland's stance/engagement + metadiscourse theory form a coherent analytical lens
- **Appropriate focus**: Correctly identifies discoursal and authorial self as key identity dimensions
- **Integration**: Shows how the frameworks complement each other

**Critical Issues Requiring Revision:**

#### Issue 3.1: Operationalization Gap
- The frameworks are described but **not fully operationalized**
- Need explicit coding categories with examples
- **Required addition**: A table showing:
  - Framework component (e.g., "Stance - Hedges")
  - Linguistic features (e.g., "modal verbs: might, could, may")
  - Example from pilot data
  - Analytical interpretation

#### Issue 3.2: AI-Specific Theoretical Considerations
- Current frameworks were developed for human writers
- **Critical question**: Do these frameworks adequately capture AI-generated discourse?
- Need discussion of potential theoretical limitations or adaptations
- Consider: Do concepts like "authorial identity" apply to AI? How do you define this?

#### Issue 3.3: Native Speaker Benchmark Assumption
- The proposal assumes native English speaker writing represents an "ideal" or "standard"
- This assumption is **problematic** and under-theorized
- Need to:
  - Acknowledge debates about native speaker privilege in academic discourse
  - Justify why native speaker texts serve as the comparison corpus
  - Consider framing as "disciplinary expertise" rather than "native-likeness"

**Recommendation**: **Mandatory revision of theoretical framework section**

---

### 4. Methodology ⚠️ **REQUIRES SUBSTANTIAL DEVELOPMENT**

This is the **most critical area requiring revision**. While the overall design is sound, multiple methodological details are unclear, underspecified, or potentially problematic.

#### 4.1 Corpus Compilation - **MAJOR CONCERNS**

##### Issue 4.1.1: Sample Size Justification
- "25-30 theses" is described as "modest" but is this sufficient?
- **Required**: Power analysis should be conducted **before** data collection, not after
- Need specific justification: Why 25-30? Based on what calculation?
- Typical corpus linguistics studies use larger samples—justify your approach

##### Issue 4.1.2: Sampling Criteria Under-Specified
- "Reputable open-access university repositories" is too vague
- **Required specifications**:
  - Which universities/repositories? (e.g., top 50 linguistics programs?)
  - What counts as "linguistics"? (Applied? Theoretical? Computational? All subfields?)
  - Time range? (Last 5 years? 10 years?)
  - Inclusion/exclusion criteria for quality?
  - How will you verify native speaker status ethically?

##### Issue 4.1.3: Native Speaker Verification - **ETHICAL CONCERN**
- How will you determine "native English speaker" status?
- This raises privacy and ethical issues
- Cannot rely on names or institutional location
- **Suggestion**: Consider reframing as "English-medium PhD theses from L1-English contexts" OR drop the native speaker requirement entirely and focus on "expert/published" vs. "AI-generated"

##### Issue 4.1.4: Chapter Selection Rationale
- Introduction and Discussion chapters are appropriate but need stronger justification
- What about Literature Review chapters (often highly evaluative)?
- What about Methodology chapters (often involve explicit positioning)?
- Consider piloting to confirm these chapters show maximum variance in stance markers

#### 4.2 AI Generation Procedure - **CRITICAL CONCERNS**

##### Issue 4.2.1: "Customized Chatbot" Under-Defined
- **This is the core innovation of your study but is insufficiently explained**
- What exactly does "customized" mean in your context?
  - Custom GPT with specific instructions?
  - Fine-tuned model on linguistics corpus?
  - RAG (Retrieval-Augmented Generation) system?
  - Prompt engineering with few-shot examples?
- **Required**: Full technical specification of your customization approach

##### Issue 4.2.2: Prompt Design - **MAJOR GAP**
- "Prompts... are designed to elicit native-like, genre-appropriate academic writing"
- **This is methodologically problematic**:
  - If you prompt for "native-like" writing, you're biasing toward your hypothesis
  - How do you ensure consistency across 25-30 generations?
  - What is your prompt iteration/refinement process?
- **Required**: 
  - Include full prompt templates in appendix
  - Explain prompt engineering methodology
  - Justify prompt choices empirically (pilot testing)

##### Issue 4.2.3: AI Model Specification
- "Such as GPT-4" is too vague for replicability
- **Required specifications**:
  - Exact model version (GPT-4-turbo? GPT-4o? Claude? Which version?)
  - Model parameters (temperature, top_p, max tokens, etc.)
  - API version and date of generation
  - Will you use same model throughout or update as new versions release?

##### Issue 4.2.4: Content Fidelity Concerns
- Basing AI generation on "title, abstract, and research questions" may not provide enough content for full chapter generation
- How do you ensure AI-generated content is **comparable** to human chapters?
  - Same research topic? Same arguments? Same literature?
- **Risk**: You might end up comparing different content rather than different authorial voices
- **Suggestion**: Consider using full human thesis as input and asking AI to "rewrite" specific chapters while maintaining content

##### Issue 4.2.5: Replication and Variability
- AI models are stochastic—will you generate multiple versions of each chapter?
- How will you handle variability in AI outputs?
- Single generation per thesis is methodologically risky

#### 4.3 Data Preparation and Coding - **GOOD WITH MINOR ISSUES**

**Strengths:**
- Appropriate use of automatic annotation
- 10% manual verification is reasonable
- Anonymization protocols mentioned

**Issues:**

##### Issue 4.3.1: Keyword Lists
- "Established keyword lists" is vague—which lists exactly?
- Need citations (Hyland's original lists? Modified versions?)
- How will you handle:
  - Discipline-specific hedges/boosters unique to linguistics?
  - False positives (e.g., "may" as month vs. hedge)?
  - New forms not in established lists?

##### Issue 4.3.2: Inter-Rater Reliability
- 10% sample is fine, but who is the second coder?
- Will you use Cohen's kappa or other metrics?
- What happens with remaining 90%—all automatic?
- Consider: 20% manual coding for higher stakes project

##### Issue 4.3.3: Self-Mention Detection
- This is notoriously difficult to automate
- First-person pronouns alone are insufficient (e.g., "We" = inclusive vs. exclusive; "I" in quotes)
- Need more sophisticated coding protocol

#### 4.4 Analytical Procedures - **SOLID FOUNDATION WITH GAPS**

**Strengths:**
- Appropriate statistical tests (Mann-Whitney U, chi-square)
- Keyness and collocation analysis will yield interesting insights
- Integration of quantitative and qualitative analysis

**Issues:**

##### Issue 4.4.1: Multiple Comparisons Problem
- Testing dozens/hundreds of linguistic features will inflate Type I error
- **Required**: Bonferroni correction or other adjustment for multiple comparisons
- OR: Justify exploratory approach with appropriate caveats

##### Issue 4.4.2: Effect Size Reporting
- Statistical significance is not enough—need effect sizes
- Report Cohen's d, Cramer's V, or other appropriate metrics
- Discuss practical vs. statistical significance

##### Issue 4.4.3: Qualitative Analysis Under-Specified
- "Selected text excerpts" — how selected? Random? Maximum variation? Extreme cases?
- Who conducts qualitative analysis? What's the procedure?
- How do you prevent confirmation bias?
- Consider: Structured qualitative protocol (e.g., thematic analysis framework)

##### Issue 4.4.4: Software and Tools
- Mention of AntConc and SketchEngine is good
- Need version numbers for replicability
- What about statistical software? (R? Python? SPSS?)
- Will code be shared?

#### 4.5 Ethical Considerations - **INSUFFICIENT**

**Current Coverage:**
- Mentions publicly accessible data ✓
- Anonymization protocols ✓
- AI transparency ✓

**Critical Gaps:**

##### Issue 4.5.1: Ethics Approval
- **No mention of institutional ethics review**
- Even with public data, you likely need IRB/ethics approval
- **Required**: Confirm you will obtain ethics approval before data collection

##### Issue 4.5.2: Copyright and Fair Use
- PhD theses have copyright holders (students or institutions)
- Is corpus analysis fair use in your jurisdiction?
- Need legal consultation or explicit permission protocol

##### Issue 4.5.3: Native Speaker Identification Ethics
- As mentioned earlier, how do you determine this without invasion of privacy?
- Can't use names, photos, or assumptions based on institution
- May need to abandon this criterion or use alternative approach (e.g., expert-validated texts)

##### Issue 4.5.4: AI Vendor Ethics
- If using commercial API (OpenAI, Anthropic), what are data retention policies?
- Are you inadvertently training their models with thesis data?
- Consider privacy implications

##### Issue 4.5.5: Broader Impact Statement Missing
- What if your findings suggest AI can perfectly mimic human authorial voice?
- Implications for detection, academic misconduct, authorship?
- Need reflective statement on potential misuse of findings

**Recommendation**: **Expand ethics section to 1-2 pages addressing all concerns**

---

### 5. Research Questions ✅ **WELL-FORMED**

**Strengths:**
- Clear, answerable, and appropriately scoped
- Logical progression from descriptive → comparative → interpretive → applied
- RQ4 on implications is particularly strong (many proposals forget this)

**Minor Suggestions:**
- RQ1: Consider adding sub-questions for specific feature categories
- RQ3: "Mediate the negotiation" is slightly awkward phrasing—simplify to "shape" or "influence"
- Consider adding RQ on variability within each corpus (are all human texts similar? All AI texts similar?)

---

### 6. Timeline and Feasibility ❌ **MISSING**

**Critical Omission:**
- No timeline provided in the proposal
- PhD research requires careful project management

**Required Addition:**
- Detailed timeline with milestones:
  - Months 1-3: Ethics approval, finalize methodology, pilot study
  - Months 4-9: Corpus compilation and AI generation
  - Months 10-15: Coding and quantitative analysis
  - Months 16-21: Qualitative analysis and interpretation
  - Months 22-30: Writing and thesis preparation
  - Months 31-36: Revisions and defense preparation

**Feasibility Concerns:**
- AI generation of 50-60 chapters (if 25-30 theses × 2 chapters) is time-intensive
- Prompt refinement alone could take months
- Corpus coding even with automation requires substantial time
- Is 3-year timeline realistic? Or planning for 4 years?

---

### 7. Resources and Training Needs ❌ **MISSING**

**Required Discussion:**
- **Computational resources**: API costs for AI generation (could be $1000s)
- **Software licenses**: SketchEngine is commercial—funded by department?
- **Training needs**: 
  - Advanced corpus linguistics methods?
  - Statistical training for corpus analysis?
  - AI/NLP training for prompt engineering?
  - Python/R programming for automation?
- **Supervision team**: One supervisor sufficient? Need co-supervisor with AI expertise?

---

### 8. Writing Quality and Presentation ✅ **EXCELLENT**

**Strengths:**
- Clear, precise academic writing
- Excellent structure and logical flow
- Appropriate use of technical terminology
- Comprehensive and properly formatted references
- Shows mature scholarly voice

**Minor Issues:**
- Few formatting inconsistencies (heading levels in Theory section)
- Some sentences are quite long—consider breaking up for readability
- Reference formatting needs final check (some inconsistencies in capitalization)

---

## Summary of Required Revisions

### MANDATORY (Must complete before registration):

1. **Expand and specify AI customization methodology** (Section 4.2)
   - Full technical specification of customization approach
   - Complete prompt templates with justification
   - Exact model specifications and parameters
   - Replication strategy

2. **Revise corpus sampling procedures** (Section 4.2.1)
   - Specific inclusion/exclusion criteria
   - Concrete list of repositories/institutions
   - Justification of sample size with power analysis
   - Alternative to "native speaker" requirement or ethical verification method

3. **Develop comprehensive ethics protocol** (Section 4.5)
   - Ethics approval plan
   - Copyright/fair use analysis
   - Privacy protections for native speaker identification
   - Broader impact statement

4. **Add project timeline** (New section)
   - Month-by-month plan for 3-4 years
   - Key milestones and deliverables
   - Risk management and contingencies

5. **Clarify theoretical operationalization** (Section 3)
   - Detailed coding framework table
   - Discussion of theoretical applicability to AI text
   - Justification or reframing of native speaker benchmark

### STRONGLY RECOMMENDED:

6. **Add pilot study section**
   - 3-5 theses pilot to test methodology
   - Prompt refinement process
   - Preliminary findings to validate approach

7. **Expand methodological literature review**
   - Corpus linguistics methods
   - AI text generation evaluation methods
   - Comparative discourse analysis precedents

8. **Specify qualitative analysis protocol**
   - Selection criteria for excerpts
   - Analytical framework (thematic? discourse-analytic?)
   - Quality assurance measures

9. **Address multiple comparisons issue**
   - Statistical correction strategy
   - Or justify exploratory approach

10. **Add resources and training section**
    - Budget estimate
    - Software and tools
    - Training/development needs

---

## Supervisor Feedback: Next Steps

### Phase 1: Pre-Registration Revisions (3-4 months)

1. **Month 1**: Conduct comprehensive pilot study
   - Select 3-5 theses
   - Develop and test AI customization approach
   - Refine prompts through iterative testing
   - Test coding framework
   - Generate preliminary data on effect sizes (informs power analysis)

2. **Month 2**: Revise methodology based on pilot
   - Finalize corpus sampling criteria
   - Complete technical specifications
   - Develop detailed coding manual
   - Conduct power analysis with pilot effect sizes

3. **Month 3**: Address theoretical and ethical issues
   - Expand theoretical framework with operationalization
   - Prepare ethics application
   - Develop resource budget
   - Create project timeline

4. **Month 4**: Submit revised proposal for registration
   - Incorporate all mandatory revisions
   - Prepare for registration interview/upgrade

### Phase 2: Registration and Year 1

- Formal PhD registration after approval
- Ethics approval obtained
- Begin full corpus compilation
- Regular monthly supervision meetings
- First-year progress review with mini-viva

---

## Additional Supervisory Guidance

### Recommended Reading to Strengthen Proposal:

**Corpus Linguistics Methods:**
- Biber, D., Connor, U., & Upton, T. A. (2007). *Discourse on the move: Using corpus analysis to describe discourse structure*
- Anthony, L. (2022). *AntConc* (Version 4.2.0) [Computer Software]
- Friginal, E. (2018). *Corpus linguistics for English teachers*

**AI Text Evaluation:**
- Mitchell, M. et al. (2019). Model cards for model reporting. *FAT* Conference
- Gebru, T. et al. (2021). Datasheets for datasets. *CACM*
- Weidinger, L. et al. (2021). Ethical and social risks of harm from language models

**Native vs. Non-Native Academic Writing:**
- Paltridge, B. (2002). Thesis and dissertation writing: An examination of published advice
- Tardy, C. M. (2009). *Building genre knowledge*
- Flowerdew, J., & Li, Y. (2007). Language re-use among Chinese apprentice scientists writing for publication

### Potential Extensions/Future Directions:

Once core study is complete, consider:
- Longitudinal study: How does AI assistance affect development of voice over time?
- Intervention study: Training PhD students in critical AI use
- Cross-disciplinary comparison: Does pattern hold in STEM vs. humanities?
- Collaboration with AI developers to create pedagogically-sound writing tools

### Warning Flags to Monitor:

- **Scope creep**: This project is ambitious—resist temptation to add more
- **Technical difficulties**: AI APIs change rapidly—have backup plans
- **Data access**: Repository policies may change—secure data early
- **Publication pressure**: Publish pilot findings to establish priority for your research gap

---

## Final Supervisor Assessment

### Would I take you as my PhD student? **YES**

**Reasoning:**

✅ **Strong intellectual capacity**: Demonstrates theoretical sophistication and critical thinking  
✅ **Timely research topic**: Addresses urgent questions in field  
✅ **Methodological ambition**: Willing to tackle complex multi-method design  
✅ **Academic writing quality**: Excellent for proposal stage  
✅ **Research independence**: Shows initiative in identifying novel research gap  

⚠️ **Conditional on**:
- Completing mandatory revisions (particularly methodology and ethics)
- Successful pilot study demonstrating feasibility
- Willingness to engage with critical feedback and refine approach
- Commitment to rigorous, transparent research practices

### Supervisory Relationship Expectations:

If you join my research group:
- **Meeting frequency**: Every 2-3 weeks during active research phases
- **Written work**: Submit written progress reports monthly
- **Presentations**: Lab group presentations every semester
- **Conferences**: Present at least 2 conferences by year 2
- **Publications**: Aim for 2-3 publications before submission
- **Teaching**: Opportunity to tutor/TA in corpus linguistics or academic writing courses

### Your Strengths as a Candidate:

1. Strong theoretical grounding in applied linguistics
2. Up-to-date with current literature (including 2024-2025 publications)
3. Identifies genuine research gap (not derivative)
4. Excellent academic writing for this stage
5. Shows methodological creativity

### Areas for Your Development:

1. Technical skills (programming, prompt engineering, AI)
2. Advanced quantitative methods (statistical power, effect sizes)
3. Research ethics protocols
4. Project management for long-term research
5. Pilot testing and iterative refinement

---

## Conclusion

This is a **promising, innovative proposal** that could make a significant contribution to our understanding of AI-assisted academic writing and authorial voice construction. With substantial methodological refinement—particularly around the AI generation procedure, corpus sampling, and ethical protocols—this research has strong potential for high-impact publications and practical applications in doctoral pedagogy.

I would be **happy to supervise this project** and help you develop it into a rigorous, publication-ready PhD thesis. However, the revisions required are substantial and non-negotiable. The pilot study phase will be critical in demonstrating that your innovative methodology is feasible and sound.

**Next action**: Schedule a meeting to discuss revision timeline and pilot study design.

---

**Supervisor Signature**: [Supervisor Name]  
**Date**: November 12, 2025  
**Institution**: [University Name]  
**Department**: Applied Linguistics / Corpus Linguistics / Educational Technology

---

## Appendix: Supervision Style and Philosophy

As your potential supervisor, I believe in:
- **Rigorous methodology**: Corpus linguistics demands precision—I will push you on technical details
- **Ethical research**: AI research raises new ethical questions—we'll navigate these together carefully
- **Intellectual independence**: I guide but don't dictate—expect to take ownership of your project
- **Publication focus**: PhD is training for research career—we'll aim to publish as we go
- **Work-life balance**: Sustainable research pace over heroic burnout
- **Constructive feedback**: I give detailed, honest feedback (like this document)—expect red ink!

If this supervision style aligns with your needs and you're ready to commit to the rigorous development required, I look forward to working with you.
