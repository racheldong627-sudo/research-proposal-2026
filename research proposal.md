**Negotiating Authorial Voice in Customized Chatbot-Supported Thesis Writing: A Corpus-Based Study of Stance and Identity in PhD Research**

1.  **Introduction**

## **Literature Review Outline**

### **2.1 Introduction to the Literature Review**

- Briefly restate the research focus and rationale.

- Explain why studying **chatbot-assisted PhD writing** and **authorial voice** matters.

- Outline the structure of this section (3--4 main strands of literature).

### **2.2 Doctoral Academic Writing and Authorial Identity**

Doctoral thesis writing represents one of the most cognitively and rhetorically demanding forms of academic discourse, requiring writers not only to demonstrate disciplinary expertise but also to construct an identifiable scholarly voice. Doctoral writers must balance adherence to disciplinary conventions with the projection of a personal stance, navigating a space where individuality and conformity coexist (Hyland, 2010). This dual demand makes the doctoral thesis a key site for investigating how authorial identity and stance are enacted through linguistic and rhetorical choices.

Early research into academic voice revealed that identity in writing is both socially situated and linguistically realized. Tang and John (1999) demonstrated that even seemingly impersonal student essays reveal layers of self-positioning through the strategic use of first-person pronouns. They proposed a typology of "selves" behind *I*, including the *representative*, *guide*, and *architect*, showing that voice in academic writing is neither monolithic nor absent but emerges from a dynamic negotiation between authorial authority and disciplinary expectations. Similarly, Charles (2003) analyzed doctoral theses in two disciplines and found that stance is often constructed not merely through modality or evaluation but also through nominal structures---particularly retrospective labels such as *This problem* or *This mystery*. Such lexical devices allow writers to encapsulate arguments and signal epistemic alignment, thereby asserting their stance as competent disciplinary members.

Hyland's (2005) framework of *stance* and *engagement* provides a widely adopted model for examining these features, distinguishing between expressions of affect, commitment, and reader interaction. Extending this framework, Hyland (2010) argued that academic writing involves "performing identity" within a disciplinary community where conventions act as both constraint and resource. His corpus analyses of leading applied linguists demonstrated how individuality surfaces within the limits of shared discourse practices---a key insight for doctoral writers who must negotiate community norms while expressing intellectual agency.

Cross-linguistic and contextual studies further reveal how institutional and publication environments shape authorial voice. Işık-Taş (2018) compared Turkish and English sociology research articles, showing that Turkish scholars publishing in international journals adopted more assertive first-person use than those writing for domestic venues, where modesty norms discouraged overt self-representation. Such findings highlight how academic identity construction is sensitive to cultural and evaluative contexts, suggesting that doctoral writers working in English as an Additional Language (EAL) often recalibrate their voice to meet global publication expectations.

More recent work continues to refine the concept of authorial voice as dialogic and socially embedded. Mhilli (2023) traced its theoretical evolution over fifty years, noting a shift from viewing voice as an individual trait to understanding it as a *dialogic construct* that reflects both agency and sociocultural positioning. Othman and Lo (2023) explored this tension in their narrative inquiry of Chinese EFL doctoral students, who struggled to assert a critical authorial stance within English-medium environments. Their findings underscore that constructing academic identity involves not only linguistic skill but also epistemological negotiation across cultural boundaries. Likewise, Ren's (2023) multi-case study confirmed that L2 writers draw on autobiographical and discoursal selves (Ivanič, 1998) to author their texts, illustrating that identity construction in doctoral writing is both agentive and constrained by institutional discourse.

Collectively, these studies illuminate how doctoral thesis writing serves as a key arena for enacting authorial identity through stance and engagement. They reveal that voice construction is a multidimensional process shaped by genre conventions, disciplinary epistemologies, and writers' sociolinguistic backgrounds. Understanding these dynamics provides a foundation for examining how emerging tools---such as AI writing assistants---may alter the ways doctoral writers negotiate authorship and disciplinary voice in increasingly hybrid academic contexts.

### **3. Corpus-Based Studies of Doctoral and Academic Writing**

Corpus-based research has significantly advanced the understanding of doctoral and academic writing, offering quantitative evidence for how expert and novice writers construct disciplinary discourse. Early foundational studies (e.g., Hyland, 2008; Charles, 2006; Weber, 2016) established that academic texts reveal consistent rhetorical and lexical patterns that vary across disciplines and genres. Charles (2006) similarly examined reporting clauses in doctoral theses and research articles, revealing how citation and evaluation work jointly to project authorial positioning---a key concern for novice academic writers. Such studies underscore the value of corpus methods in identifying the linguistic means through which scholars construct both epistemic stance and identity.

More recent corpus-based studies have extended these approaches to doctoral-level texts, examining how emerging researchers acquire disciplinary conventions. Işık-Taş (2008) and Küçükoğlu (2016) compared PhD theses with research articles in English Language Teaching and International Relations, respectively. While Işık-Taş (2008) highlighted doctoral writers\' reliance on explicit metadiscourse, Küçükoğlu (2016) found that, despite both genres following the CARS model, research article introductions were lexically denser and employed author presence markers differently than those in theses. Their findings point to genre-specific variations, suggesting that thesis writing functions as an apprenticeship genre---bridging student and professional academic communities.Pathan et al. (2018), through a corpus of doctoral theses across scientific disciplines, analyzed the coverage of the Academic Word List (AWL) and revealed variations that reflect writers' differing levels of lexical sophistication and field-specific norms.

A strong body of research has focused on the linguistic building blocks of doctoral writing, particularly lexical bundles and stance markers, to understand how authorial identity is constructed. While Liu, Hu, and Liu (2022) found that cultural background influenced the type of stance markers preferred by Chinese and American students, studies on lexical bundles reveal more fundamental differences. Bao and Liu (2022) demonstrated that Chinese L2 writers relied on a limited set of formulaic sequences (e.g., \"the present study\") and avoided authorial first-person pronouns, which were prevalent in the writing of their American counterparts. Similarly, Malik et al. (2023) found that linguistics dissertation abstracts from Pakistani universities exhibited a low frequency and limited structural variety of 4-word bundles (e.g., \"in the field of\"). Collectively, these studies suggest that while rhetorical control may be discipline-wide, the specific phraseological choices of writers---marked by formulaicity and a reluctance to assert a prominent authorial self---are often culturally mediated.

Lo, Othman, and Lim (2020) analyzed metadiscourse use among Malaysian first-year ESL doctoral students and found a predominant use of textual resources (e.g., transitions, evidentials), whereas the engagement dimension (e.g., writer-oriented markers) was the least employed. This suggests that novice writers prioritize textual coherence over persuasive engagement, a pattern consistent with findings from other EFL doctoral contexts. Collectively, such corpus-based studies demonstrate how stance, engagement, and lexical sophistication interact to form the rhetorical signature of doctoral writing.

Several large-scale corpora have supported these analyses, such as the *British Academic Written English Corpus* (BAWE) and the *Michigan Corpus of Upper-level Student Papers* (MICUSP), both of which include postgraduate writing samples across disciplines. However, relatively few corpora focus specifically on PhD theses, particularly in linguistics or EFL contexts. This lack of doctoral-level linguistic data constrains our understanding of how emerging scholars construct disciplinary voice and how new technologies---such as customized chatbots---might reshape these patterns.

In sum, corpus-based research on doctoral writing has illuminated how rhetorical and lexical features contribute to disciplinary identity construction. Yet, despite growing interest in stance and metadiscourse, there remains limited exploration of how AI-assisted or chatbot-supported writing might influence these linguistic patterns. Addressing this gap through a specialized corpus comparing human-authored and chatbot-assisted PhD texts in linguistics would extend existing corpus methodologies and offer valuable insights into the evolving ecology of academic writing.

### **4. AI and Chatbot Support in Academic Writing**

The emergence of artificial intelligence (AI) writing assistants and conversational agents has redefined academic writing practices across higher education. Early tools such as **Grammarly** and **Quillbot** offered automated grammar correction and paraphrasing, paving the way for advanced systems like **ChatGPT**, which leverage large language models (LLMs) to generate, refine, and critique text. This technological shift has expanded access to writing support for multilingual and EFL learners, doctoral writers, and educators, while also raising questions about authenticity, cognitive engagement, and ethical use (Memarian & Doleck, 2023; Kasneci et al., n.d.).

Systematic reviews consistently report that ChatGPT's integration into education enhances learning performance and writing fluency. **Deng et al. (2025)** found through a meta-analysis of 69 experimental studies that ChatGPT interventions improved academic performance, higher-order thinking, and affective motivation across university contexts. Similarly, **Albadarin et al. (2024)** observed that students use ChatGPT to generate ideas, summarize readings, clarify concepts, and refine written drafts, leading to improved coherence and linguistic accuracy. These findings are echoed in **Abdallah et al. (2025)** and **Munaye et al. (2025)**, who concluded that LLMs facilitate cognitive and emotional support, foster collaboration, and enhance accessibility, although overreliance on AI-generated content may weaken independent learning and critical thinking.

Empirical work focusing on **EFL and ESL learners** further substantiates these benefits. In Indonesia, **Marzuki et al. (2023)** reported that AI writing tools enhanced students' content organization and writing quality, while **Mahapatra (2024)** demonstrated that ChatGPT-based formative feedback significantly improved ESL students' academic writing skills and engagement. Likewise, **Mekheimer (2025)** found that AI-assisted feedback using Grammarly led to measurable gains in writing proficiency and revision practices among postgraduate EFL students. Collectively, these studies underscore AI's pedagogical value as a supportive, dialogic tool that promotes learner autonomy when used with instructional guidance.

Nevertheless, concerns persist regarding **accuracy, originality, and ethical integrity**. Studies by **Gao et al. (2023)** and **Desaire et al. (2023)** reveal that ChatGPT-generated academic texts can be highly convincing but occasionally contain fabricated data or overly formulaic phrasing. This indistinguishability from human writing underscores the urgent need for transparent authorship policies and reliable AI detection methods. Broader reviews (Abdallah et al., 2025; Kasneci et al., n.d.) highlight risks such as technostress, cognitive offloading, and diminished critical engagement, suggesting that AI literacy and ethical frameworks are essential for sustainable integration.

Recent developments emphasize **customization and fine-tuning** as a promising frontier for academic writing support. **Kabir et al. (2025)** demonstrated the potential of personalized GPT models trained for scientific writing---such as the *Neurosurgical Research Paper Writer*---to improve efficiency and accuracy in manuscript preparation. However, the study also noted persistent fallacies and the need for calibration to maintain reliability. Such innovations point toward a future of discipline-specific, customizable AI assistants capable of supporting doctoral and EFL writers in developing coherent academic voice and stance, provided their use is accompanied by critical oversight and reflective pedagogy.

In sum, the integration of AI and chatbots in academic writing presents a dual narrative of empowerment and caution. While empirical and systematic evidence highlights their capacity to enhance fluency, coherence, and idea generation, the unresolved tensions around originality, ethics, and dependency demand further investigation. This study responds to that gap by examining how **customized AI chatbots** can support doctoral EFL writers in refining rhetorical stance and authorial identity without diminishing academic integrity.

### **5. Negotiating Authorial Voice in AI-Assisted Writing**

The rapid diffusion of generative artificial intelligence (AI) such as ChatGPT has generated both enthusiasm and apprehension regarding its impact on authorial voice, originality, and ownership in academic writing. A growing body of comparative and conceptual research has begun to explore the linguistic, rhetorical, and ethical dimensions of human--AI text production. Together, these studies highlight the potential of AI to emulate academic style while underscoring its continuing limitations in reproducing disciplinary voice and the nuanced self-representation that characterize human scholarly authorship.

Empirical comparisons between human- and AI-generated academic texts reveal both convergence and divergence at the rhetorical and lexical levels. In medical and scientific domains, Gao et al. (2023) found that abstracts produced by ChatGPT closely resembled authentic journal abstracts in fluency and structure, yet they tended to be vaguer and more formulaic. Similarly, Desaire et al. (2023) demonstrated that even accessible machine-learning tools could distinguish ChatGPT writing from human science prose with over 99 % accuracy, largely due to its shorter paragraphs and reduced use of hedging devices such as *however* and *although*. In humanities contexts, Revell et al. (2024) reported that essays generated by ChatGPT achieved comparable grades to first-year English undergraduates, but human writers displayed greater interpretive depth, contextual awareness, and stylistic variability. Sardinha (2024) extended this contrast through a multidimensional corpus analysis, showing that AI-generated texts diverged significantly from human writing along Biber's five linguistic dimensions---particularly in features of involvement, elaboration, and information density---thus confirming the persistent distinctiveness of human rhetorical patterning.

From an authorship perspective, studies have problematized whether AI can possess or simulate authorial presence. Amirjalili, Neysani, and Nikbakht (2024) compared a student's essay with one written by ChatGPT and found that while AI produced grammatically sound and contextually relevant prose, it lacked assertiveness, self-identification, and depth of authorial presence. Such deficiencies point to what might be called *attenuated authorship*: text that is linguistically proficient yet devoid of epistemic stance or personal investment. The challenge, therefore, lies not only in distinguishing AI text but in theorizing how human writers negotiate or reclaim their authorial agency when using AI tools as collaborators.

Recent technological developments have further blurred the boundary between tool and co-author. Kabir et al. (2025) demonstrated that **customized GPTs**---fine-tuned through iterative prompt design---can produce discipline-specific manuscripts resembling those authored by professionals. Their findings foreshadow a move toward *hybrid authorship*, in which scholars co-construct texts with AI systems that encode disciplinary conventions. Yet, even with customized models, factual inaccuracies and rhetorical rigidity persist, suggesting that human oversight remains essential to preserve disciplinary authenticity and ethical integrity.

This emergent hybridity raises profound questions of accountability and authorship attribution. Lund and Naheem (2024) surveyed 300 leading academic journals and found that while most prohibit listing AI as an author, many now require disclosure of AI assistance in acknowledgments. Such policies reflect an evolving consensus that AI may participate in, but cannot assume responsibility for, scholarly authorship. Collectively, these studies point to a transitional stage in academic writing: AI can efficiently emulate form and coherence, but the construction of stance, engagement, and disciplinary identity remains a distinctly human act.

Despite these advances, no empirical study has yet examined how *customized* chatbot-assisted writing influences the formation of authorial voice and stance in doctoral theses---particularly within linguistics. Understanding this negotiation of identity and authorship in AI-mediated doctoral writing represents a crucial next step for applied linguistics and EAP research.

### **6. Summary and Identified Research Gaps**

- Synthesize what is known and unknown.

- Identify **three converging gaps**:

1.  Lack of corpus-based comparison between chatbot-assisted and human-authored doctoral texts.

2.  Limited understanding of AI's effect on authorial stance and identity construction.

3.  Absence of discipline-specific (Linguistics) analysis of AI co-authorship at doctoral level.

4.  Conclude by justifying the **need and originality** of your proposed study.

<!-- -->

3.  **Theoretical Foundation**

This study is grounded in theoretical perspectives on authorial identity, stance, and metadiscourse in academic writing, providing an integrated framework for analyzing how authorial voice is linguistically realized in doctoral theses and AI-generated academic texts. The framework synthesizes key concepts from Ivanič's (1998) model of writer identity, Hyland's (2005, 2008) stance and engagement framework, and metadiscourse theory, all of which offer valuable tools for understanding how writers---or in this case, human and AI systems---construct presence and authority in text.

### **3.1 Authorial Identity and Voice Construction**

Ivanič's (1998) theory of writer identity distinguishes among several interrelated aspects of authorship, including the *autobiographical self* (the writer's personal history), the *discoursal self* (the persona projected through language), and the *authorial self* (the position adopted in relation to the content and audience). In doctoral writing, the *discoursal self* and *authorial self* are particularly salient, as writers must align personal positioning with disciplinary conventions and epistemic norms.

In the current study, Ivanič's framework underpins the analysis of how "voice" and "self" are represented linguistically in human-authored doctoral texts and whether AI-generated writing reproduces or alters these discursive identities.

### **3.2 Stance and Engagement Framework**

Building on this, Hyland's (2005, 2008) model of stance and engagement provides a functional taxonomy for analyzing the interpersonal dimension of academic discourse. Stance markers (e.g., hedges, boosters, attitude markers, and self-mentions) convey the writer's affective and epistemic positioning, while engagement markers (e.g., reader pronouns, questions, and directives) signal awareness of and interaction with readers. This framework enables a systematic comparison of how doctoral writers and AI-generated texts differ in expressing commitment, confidence, and authorial presence---key indicators of voice and identity in academic genres.

### **3.3 Metadiscourse as a Marker of Voice**

Metadiscourse theory (Hyland, 2005; Hyland & Tse, 2004) complements the stance/engagement framework by conceptualizing writing as a social act that constructs both relationships and meanings. Through metadiscursive resources, writers manage the flow of information and interaction with readers, crafting a credible scholarly persona. In corpus-based analysis, these resources can be quantified and qualitatively interpreted to reveal differences between human and AI-produced texts in the rhetorical projection of authorial control and persuasion.

### **3.4 Integrative Application to Corpus-Based Analysis**

Together, these theoretical perspectives provide a fine-grained framework for analyzing the linguistic and rhetorical realization of authorial identity. In this study, they inform the corpus-assisted discourse analysis of two text types: (1) doctoral thesis excerpts authored by human writers (drawn from established academic writing corpora), and (2) parallel AI-generated texts produced through chatbot prompting. The analysis focuses on how stance markers, self-mentions, and metadiscursive elements collectively construct---or fail to construct---a distinct authorial presence. This theoretical integration allows for an exploration of how AI text generation might reshape or dilute the rhetorical patterns through which doctoral authors traditionally enact identity and disciplinary authority.

## **4. Methodology**

### **4.1 Research Design**

This study employs a **comparative corpus-based design** that integrates **quantitative corpus-linguistic analysis** with **qualitative discourse interpretation**. The goal is to examine how authorial voice and stance are constructed in human-authored linguistics PhD theses compared with AI-generated equivalents. The corpus approach enables systematic examination of linguistic patterns across large text samples, while the discourse-analytic lens allows for a nuanced interpretation of authorial identity and stance realization within disciplinary contexts.

### **4.2 Corpus Compilation**

The corpus will comprise two comparable sub-corpora:

1.  **Human-authored corpus** -- consisting of approximately 25--30 linguistics PhD theses written by doctoral students, collected from institutional repositories (e.g., university open-access thesis databases).

2.  **AI-generated corpus** -- composed of AI-produced versions of the same thesis sections, generated via a customized chatbot using each thesis's title, abstract, and research questions as input prompts.

Only two sections---**Introductions** and **Discussions**---will be extracted from both human-authored and AI-generated texts, as these are the most **identity-rich and argumentative** parts of doctoral writing. Prior studies have shown that these sections demand strong rhetorical positioning and explicit self-representation, making them particularly suitable for analyzing stance and voice (Swales, 2004; Hyland, 2005; Charles, 2003). Metadata such as author L1, year, and institution will be recorded for contextual interpretation.

### **4.3 AI Text Generation**

To ensure controlled and reproducible outputs, AI texts will be generated using a **customized chatbot** configured to simulate EFL academic writing style. Each AI text will correspond to a specific human-authored thesis, maintaining parallel structure and content scope (Introduction and Discussion). Generation logs, model specifications, and input parameters will be documented to ensure methodological transparency and replicability.

### **4.4 Data Preparation and Coding Framework**

All text data will be cleaned by removing references, tables, figures, and formatting. The sections will be converted into plain-text format for corpus analysis. Coding will follow two major frameworks:

- **Hyland's (2005) Metadiscourse Model**, which identifies two major categories:

<!-- -->

- *Stance markers* (hedges, boosters, attitude markers, self-mentions)

- *Engagement markers* (reader pronouns, directives, questions)

<!-- -->

- **Authorial Identity Markers** (Ivanič, 1998), focusing on expressions of self-positioning, epistemic modality, and interactional alignment.

Manual validation will be applied to refine automatic tagging results, with intra-coder reliability checks performed on a subset of texts.

### **4.5 Analytical Procedures**

The analysis will involve both **quantitative** and **qualitative** components.

First, **quantitative corpus-linguistic analysis** will measure the frequency and distribution of stance and engagement markers across the two corpora. Statistical comparisons (e.g., chi-square tests and Mann--Whitney U tests) will identify significant differences between human and AI-authored texts. Keyness and collocation analyses (using *AntConc* and *SketchEngine*) will reveal distinctive lexical and phraseological patterns.

Second, **qualitative discourse analysis** will interpret these patterns in relation to the construction of authorial identity. Particular attention will be paid to how writers---or AI systems---negotiate epistemic commitment, interpersonal engagement, and self-representation through linguistic choices. Selected excerpts will be analyzed in depth to exemplify divergences in stance realization and disciplinary voice.

## **5. Implications and Contributions to Knowledge**

This study is positioned at the intersection of **doctoral writing research, corpus linguistics, and AI-assisted academic authorship**. Its contributions extend across three main domains: **theoretical advancement**, **methodological innovation**, and **pedagogical and ethical implications**.

### **5.1 Theoretical Contributions**

Theoretically, the study extends existing models of authorial identity and stance (e.g., Hyland, 2005; Ivanič, 1998) into the emerging domain of **AI-mediated writing**. While previous research has examined how human writers construct voice through stance and engagement markers, little is known about how these resources are realized when academic discourse is co-constructed with artificial intelligence. By comparing human-authored and AI-generated doctoral texts, this study will reveal whether current theories of metadiscourse and authorial voice adequately account for **hybrid authorship**, where human and machine contributions intertwine.

Furthermore, the study contributes to the ongoing debate surrounding **authorship and agency in AI-assisted writing** (????), offering an evidence-based linguistic perspective on what it means to "author" a text in an age of generative tools. Findings are expected to refine theoretical understandings of **voice negotiation** and **disciplinary positioning**, particularly within the high-stakes context of doctoral writing.

### **5.2 Methodological Contributions**

Methodologically, this project proposes an **integrated corpus--discourse approach** for studying authorial identity in AI-influenced writing. The design combines corpus-based quantitative analysis (frequency, keyness, collocation) with discourse-analytic interpretation of rhetorical stance and identity markers. This mixed approach not only ensures empirical robustness but also provides a replicable model for future inquiries into AI-mediated discourse.

The construction of a **parallel corpus**---comprising human-authored and AI-generated PhD thesis sections---will serve as a valuable resource for further research on academic voice, digital rhetoric, and AI authorship. Such a dataset will be one of the first to systematically align human doctoral writing with machine-generated equivalents, enabling cross-comparison of linguistic, rhetorical, and epistemic features.

### **5.3 Pedagogical and Ethical Implications**

From a pedagogical standpoint, the study offers practical insights for **doctoral writing instruction in EFL and EMI contexts**. By identifying how AI tools may alter stance, hedging, or engagement practices, supervisors and writing instructors can better guide doctoral students in maintaining a **credible and disciplinary voice** when using digital assistance. The findings will inform the development of **AI literacy frameworks** that help emerging scholars critically evaluate and ethically integrate AI-generated suggestions into their academic work.

Ethically, the study contributes to discussions on **authorship integrity** and **academic accountability**. As AI-generated writing becomes increasingly sophisticated, distinguishing between "support" and "substitution" of the author's intellectual contribution becomes vital. The results of this research will help clarify boundaries between legitimate assistance and inappropriate reliance, supporting the creation of institutional policies for responsible AI use in higher-degree research.

### **5.4 Overall Contribution to Knowledge**

Overall, this study addresses a significant and timely gap in applied linguistics and academic writing research by providing empirical evidence on how AI affects **authorial identity construction** in advanced scholarly genres. It bridges theoretical perspectives on stance and metadiscourse with emerging debates about **AI authorship and voice co-construction**, offering a linguistically grounded framework for understanding human--machine collaboration in doctoral writing.

Through this, the research contributes to shaping a **new paradigm of authorship**---one that recognizes the evolving dynamics of agency, identity, and disciplinary discourse in the age of intelligent writing technologies.

**References**

Abdallah, N., Katmah, R., Khalaf, K., & Jelinek, H. F. (2025). Systematic review of ChatGPT in higher education: Navigating impact on learning, wellbeing, and collaboration. *Social Sciences & Humanities Open*, *12*, 101866. <https://doi.org/10.1016/j.ssaho.2025.101866>

Albadarin, Y., Saqr, M., Pope, N., & Tukiainen, M. (2024). A systematic literature review of empirical research on ChatGPT in education. *Discover Education*, *3*(1), 60. <https://doi.org/10.1007/s44217-024-00138-2>

Amirjalili, F., Neysani, M., & Nikbakht, A. (2024). Exploring the boundaries of authorship: A comparative analysis of AI-generated text and human academic writing in English literature. *Frontiers in Education*, *9*, 1347421. <https://doi.org/10.3389/feduc.2024.1347421>

Bao, K., & Liu, M. (2022). A corpus study of lexical bundles used differently in dissertations abstracts produced by Chinese and American PhD students of linguistics. Frontiers in Psychology, 13, 893773.

Berber Sardinha, T. (2024). AI-generated vs human-authored texts: A multidimensional comparison. *Applied Corpus Linguistics*, *4*(1), 100083. <https://doi.org/10.1016/j.acorp.2023.100083>

Charles, M. (2003). 'This mystery...': A corpus-based study of the use of nouns to construct stance in theses from two contrasting disciplines. *Journal of English for Academic Purposes, 2*(4), 313--326. <https://doi.org/10.1016/S1475-1585(03)00048-1>

Charles, M. (2006). The construction of stance in reporting clauses: A cross-disciplinary study of theses. *Applied Linguistics, 27*(3), 492--518. https://doi.org/10.1093/applin/aml021

Deng, R., Jiang, M., Yu, X., Lu, Y., & Liu, S. (2025). Does ChatGPT enhance student learning? A systematic review and meta-analysis of experimental studies. *Computers & Education*, *227*, 105224. <https://doi.org/10.1016/j.compedu.2024.105224>

Desaire, H., Chua, A. E., Isom, M., Jarosova, R., & Hua, D. (2023). Distinguishing academic science writing from humans or ChatGPT with over 99% accuracy using off-the-shelf machine learning tools. *Cell Reports Physical Science*, *4*(6), 101426. <https://doi.org/10.1016/j.xcrp.2023.101426>

Gao, C. A., Howard, F. M., Markov, N. S., Dyer, E. C., Ramesh, S., Luo, Y., & Pearson, A. T. (2023). Comparing scientific abstracts generated by ChatGPT to real abstracts with detectors and blinded human reviewers. *Npj Digital Medicine*, *6*(1), 75. <https://doi.org/10.1038/s41746-023-00819-6>

Hyland, Ken. 2005. Metadiscourse. London: Continuum

Hyland, K. (2005). Stance and engagement: A model of interaction in academic discourse. *Discourse Studies, 7*(2), 173--192.https://doi.org/10.1177/1461445605050365

Hyland, K. (2008). Disciplinary voices: Interactions in research writing. English text construction, 1(1), 5-22.

Hyland, K. (2008). Academic clusters: Text patterning in published and postgraduate writing. *International Journal of Applied Linguistics*, *18*(1), 41--62. <https://doi.org/10.1111/j.1473-4192.2008.00178.x>

Hyland, K. (2010). Community and individuality: Performing identity in applied linguistics. *Written Communication, 27*(2), 159--188. https://doi.org/10.1177/0741088310364939

Hyland, K., & Tang, R. (2021). *Disciplinary identity and academic writing.* Routledge.

Hyland, K., & Tse, P. (2004). Metadiscourse in academic writing: A reappraisal. Applied Linguistics, 25(2), 156--177.

Işık-Taş, E. E. (2018). Authorial identity in Turkish language and English language research articles in sociology: The role of publication context in academic writers' discourse choices. *English for Specific Purposes, 49*, 26--38. <https://doi.org/10.1016/j.esp.2017.10.001>

Ivanič, R. (1998). Writing and identity (Vol. 10). Amsterdan: John Benjamins Publishing Company.

Kabir, A., Shah, S., Haddad, A., & Raper, D. M. S. (2025). Introducing Our Custom GPT: An Example of the Potential Impact of Personalized GPT Builders on Scientific Writing. *World Neurosurgery*, *193*, 461--468. <https://doi.org/10.1016/j.wneu.2024.10.041>

Kasneci, E., Sessler, K., Fischer, F., Gasser, U., & Groh, G. (n.d.). *ChatGPT for Good? On Opportunities and Challenges of Large Language Models for Education*.

Küçükoğlu, E. S. (2016). *A corpus-based analysis of genre-specific discourse of research in the phd theses and research articles in international relations* (Doctoral dissertation, Middle East Technical University (Turkey)).

Lund, B. D., & Naheem, K. T. (2024). Can ChatGPT be an author? A study of artificial intelligence authorship policies in top academic journals. *Learned Publishing*, *37*(1), 13--21. <https://doi.org/10.1002/leap.1582>

Liu, Y., Hu, X., & Liu, J. (2022). A corpus-based study on Chinese and American students\' rhetorical moves and stance features in dissertation abstracts. *Frontiers in Psychology, 13*, 1004744.

Mahapatra, S. (2024). Impact of ChatGPT on ESL students' academic writing skills: A mixed methods intervention study. *Smart Learning Environments*, *11*(1), 9. <https://doi.org/10.1186/s40561-024-00295-9>

Mekheimer, M. (2025). Generative AI-assisted feedback and EFL writing: A study on proficiency, revision frequency and writing quality. *Discover Education*, *4*(1), 170. <https://doi.org/10.1007/s44217-025-00602-7>

Malik, M. Z. A., Kharal, A. A., & Shehzadi, K. (2023). A Corpus-based Study of the Lexical Bundles in the Ph. D Linguistics Dissertations'Abstracts in Pakistani Universities. *Pakistan Journal of Humanities and Social Sciences, 11*(2), 2254-2262.

Marzuki, Widiati, U., Rusdin, D., Darwin, & Indrawati, I. (2023). The impact of AI writing tools on the content and organization of students' writing: EFL teachers' perspective. *Cogent Education*, *10*(2), 2236469. <https://doi.org/10.1080/2331186X.2023.2236469>

Memarian, B., & Doleck, T. (2023). ChatGPT in education: Methods, potentials, and limitations. *Computers in Human Behavior: Artificial Humans*, *1*(2), 100022. <https://doi.org/10.1016/j.chbah.2023.100022>

Mhilli, O. (2023). Authorial voice in writing: A literature review. *Social Sciences & Humanities Open, 8*(1), 100550. https://doi.org/10.1016/j.ssaho.2023.100550

Munaye, Y. Y., Admass, W., Belayneh, Y., Molla, A., & Asmare, M. (2025). ChatGPT in Education: A Systematic Review on Opportunities, Challenges, and Future Directions. *Algorithms*, *18*(6), 352. <https://doi.org/10.3390/a18060352>

Othman, J., & Lo, Y. Y. (2023). Constructing academic identity through critical argumentation: A narrative inquiry of Chinese EFL doctoral students' experiences. *SAGE Open, 13*(4). <https://doi.org/10.1177/21582440231200700>

Pathan, H., Memon, R. A., Memon, S., Shah, S. W. A., & Magsi, A. (2018). Academic vocabulary use in doctoral theses: A corpus-based lexical analysis of academic word list (AWL) in major scientific disciplinary groups. *International Journal of English Linguistics, 8*(4), 282-288.

Ren, B. (2023). *L2 writer identity construction in academic written discourse: A multi-case study* (Publication No. 30491449) \[Doctoral dissertation, University of South Florida\]. *ProQuest Dissertations and Theses Global*.

Revell, T., Yeadon, W., Cahilly-Bretzin, G., Clarke, I., Manning, G., Jones, J., Mulley, C., Pascual, R. J., Bradley, N., Thomas, D., & Leneghan, F. (2024). ChatGPT versus human essayists: An exploration of the impact of artificial intelligence for authorship and academic integrity in the humanities. *International Journal for Educational Integrity*, *20*(1), 18. <https://doi.org/10.1007/s40979-024-00161-8>

Swales, J. (2004). *Research genres: Explorations and applications.* Cambridge University Press.

Taki, S., & Jafarpour, F. (2012). Engagement and stance in academic writing: A study of English and Persian research articles. *Mediterranean Journal of Social Sciences, 3*(1), 157--167.

Tang, R., & John, S. (1999). The 'I' in identity: Exploring writer identity in student academic writing through the first person pronoun. *English for Specific Purposes, 18*, S23--S39. <https://doi.org/10.1016/S0889-4906(99)00009-5>

Taş, E. E. I. (2008). *A corpus-based analysis of genre-specific discourse of research: The PhD thesis and the research article in ELT* (Doctoral dissertation, Middle East Technical University (Turkey)).

Weber, B. (2016). Signalling nouns in english. a corpus-based discourse approach. *ICAME Journal, 40*(1), 160-165.

Wu, D., & Buripakdi, A. (2022). Writer identity construction in EFL doctoral thesis writing: A narrative study. *Writing & Pedagogy, 13*(1--2), 67--91. <https://doi.org/10.1558/wap.20669>

Yea, L. Y., Othman, J., & Wei, L. J. (2020). The use of metadiscourse in academic writing by Malaysian first-year ESL doctoral students. *Indonesian Journal of Applied Linguistics, 10*(1), 271-282.

Zhang, L., & Wang, J. (2024). To be like a "scholar": A study on the construction of authorial identity of Chinese EFL learners in academic writing---An intertextuality perspective. *Frontiers in Psychology, 15*, 1322851. <https://doi.org/10.3389/fpsyg.2024.1322851>
